from datetime import datetime
import os
import threading
import cv2
import numpy as np
import tkinter as tk
from tkinter import filedialog, messagebox
from collections import defaultdict
from validation.manual_validator import validate_event
from export.csv_export import export_events_to_csv

# Professional error handling and logging
try:
    from utils.error_handling import get_logger, setup_error_handling, PerformanceMonitor
except ImportError:
    # Fallback for environments without error handling module
    class DummyLogger:
        def info(self, msg): pass  # Removed console output for cleaner interface
        def warning(self, msg): pass  # Removed console output for cleaner interface
        def error(self, msg, exception=None): pass  # Removed console output for cleaner interface
        def debug(self, msg): pass
    
    def get_logger(): return DummyLogger()
    def setup_error_handling(callback=None): return None
    class PerformanceMonitor:
        def __init__(self, logger): pass
        def record_frame_time(self, t): pass
        def record_error(self): pass
        def get_performance_summary(self): return {}
        def log_performance_summary(self): pass

def safe_messagebox(func, title, message, fallback_status=None):
    """
    Safe wrapper for messagebox functions that handles interruption gracefully.
    
    Args:
        func: messagebox function (showinfo, showwarning, showerror)
        title: Dialog title
        message: Dialog message
        fallback_status: Optional status message to show if dialog fails
    """
    try:
        return func(title, message)
    except (KeyboardInterrupt, tk.TclError):
        # Handle user interruption or window closing gracefully
        # Removed console output for cleaner interface
        if fallback_status and hasattr(safe_messagebox, 'gui_instance'):
            safe_messagebox.gui_instance.update_status(fallback_status)
    except Exception as e:
        # Handle any other unexpected errors
        # Removed console output for cleaner interface  
        if fallback_status and hasattr(safe_messagebox, 'gui_instance'):
            safe_messagebox.gui_instance.update_status(fallback_status)
from visualization.visualization import export_flightMap
from export.video_export import export_video
import glob
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
import queue
import time

from utils.config import (
    MIN_CONTOUR_AREA, MAX_CONTOUR_AREA,
    NOISE_KERNEL, STABILIZATION_WINDOW,
    COOLDOWN_FRAMES,
    USE_PARALLEL_CONTOUR,  # Added contour processing configuration
    # Polygon visualization configuration
    POLYGON_ACTIVE_COLOR, POLYGON_INACTIVE_COLOR,
    POLYGON_OUTLINE_THICKNESS_ACTIVE, POLYGON_OUTLINE_THICKNESS_INACTIVE,
    POLYGON_OVERLAY_ALPHA, POLYGON_ENABLE_OVERLAY, POLYGON_ENABLE_INFO_PANEL,
    BAT_DETECTION_COLOR, BAT_CENTER_COLOR, BAT_CROSSHAIR_COLOR,
    BAT_DETECTION_RADIUS, BAT_CENTER_RADIUS, BAT_OUTER_RING_RADIUS,
    POLYGON_LABEL_FONT, POLYGON_LABEL_SCALE, POLYGON_LABEL_THICKNESS,
    BAT_LABEL_FONT, BAT_LABEL_SCALE, BAT_LABEL_THICKNESS,
    INFO_PANEL_BACKGROUND_COLOR, INFO_PANEL_TEXT_COLOR, INFO_PANEL_ALPHA,
    INFO_PANEL_WIDTH, INFO_PANEL_POSITION
)

# Import German video overlay system
from utils.video_overlay_system import VideoDisplayManager



class VideoDetector:
    def __init__(self, gui):
        self.gui = gui
        
        # Professional error handling and logging system
        self.logger = get_logger()
        self.error_handler = None  # Will be initialized after thread-safe methods
        self.performance_monitor = PerformanceMonitor(self.logger)
        
        # Log initialization
        self.logger.info("VideoDetector initializing...")
        
        # ðŸ”§ CRITICAL FIX: Set GUI reference for safe messagebox handling
        safe_messagebox.gui_instance = gui
        
        # ðŸŽ¥ VIDEO OVERLAY SYSTEM: Initialize German overlay display
        self.video_display = VideoDisplayManager("Fledermaus-Erkennung")
        self.show_overlay = True  # Enable/disable real-time overlay
        self.recent_detections = {}  # Track recent detections for polygon highlighting
        self.detection_timeout = 60  # Frames to keep polygon highlighted after detection
        
        self.cap = None
        self.fps = 0
        self.total_frames = 0
        self.processing = False
        self.back_sub = self._create_optimized_background_subtractor()
        self.motion_history = []
        self.events = []
        self.marked_frames = []
        self.event_frames = {}  # NEW: Store actual frames for each event
        self.bat_centers = []
        
        #  bat tracking system for polygon mode
        self.bat_tracker = {}  # Dictionary: {bat_id: {'position': (x,y), 'last_seen': frame_number, 'polygon_idx': idx}}
        self.next_bat_id = 1  # Counter for unique bat IDs
        self.bat_tracking_threshold = 50  # Max pixel distance to consider same bat
        self.bat_tracking_timeout = 30   # Frames after which a bat ID expires
        
        # Enhanced tracking for optimized detection
        self.enhanced_bat_tracker = {}  # Advanced tracking for enhanced detection
        self.max_bat_distance = 50  # pixels for bat tracking
        self.bat_id_counter = 0
        self.active_bats = {}  # Track bats across frames with timeout
        
        self.cooldown_counter = 0
        self.bat_inside = False
        self.roi = None  # (x, y, w, h)
        self.user_roi = None  # Track the actual user-drawn ROI (separate from calculated polygon bounding ROI)
        self.prev_gray = None
        self.video_path = None
        self.polygon_areas = []  # List of polygon areas for filtering
        self.use_polygon_filtering = False  # Flag to control polygon filtering
        
        # Priority handling attributes
        self.active_detection_mode = None  # Track current detection mode
        self.active_area_info = {}  # Store info about active detection area
        
        # Polygon masking attributes
        self.polygon_mask = None  # Binary mask for polygon areas
        self.frame_shape = None  # Store frame dimensions for mask creation
        
        # OPTIMIZATION: Performance enhancement attributes
        self.polygon_bounding_rect = None  # Bounding rectangle for all polygons combined
        self.polygon_areas_cv2 = []  # Pre-converted polygons for cv2.pointPolygonTest (faster)
        self.use_bounding_rect_optimization = True  # Enable bounding rectangle optimization
        
        # ENHANCED DETECTION: FastPolygonEngine integration
        self.dedicated_back_sub = None  # Dedicated background subtractor for enhanced detection
        self.polygon_masks = []  # Binary masks for each polygon
        self.expanded_polygon_masks = []  # Edge-expanded masks for fast-moving bats
        self.combined_polygon_mask = None  # Combined mask for all polygons
        self.expanded_combined_mask = None  # Combined expanded mask
        self.edge_expansion = 6  # pixels for edge expansion
        self.overlap_threshold = 0.25  # 25% minimum overlap for detection
        self.use_enhanced_detection = True  # Enable FastPolygonEngine-based detection
        
        # THREADING OPTIMIZATION: Configure OpenCV for maximum performance
        self._configure_opencv_threading()
        
        # FRAME PROCESSING QUEUE: Performance optimization with threading
        self.use_frame_queue = True  # Enable frame queue processing
        self.frame_queue_size = 5    # Number of frames to queue ahead
        self.processing_threads = min(4, os.cpu_count())  # Max worker threads
        self.frame_queue = queue.Queue(maxsize=self.frame_queue_size)
        self.result_queue = queue.Queue()
        self.processing_pool = None
        
        # PERFORMANCE MONITORING: Track processing metrics
        self.performance_metrics = {
            'total_frames_processed': 0,
            'total_processing_time': 0.0,
            'contour_processing_times': [],
            'frame_processing_times': [],
            'threading_enabled': True,
            'start_time': None,
            'memory_cleanup_counter': 0,  # Track memory cleanup cycles
            'last_gc_frame': 0           # Last frame when garbage collection was performed
        }

        # LONG VIDEO OPTIMIZATION: Enhanced memory management for 1-3 hour videos
        self.long_video_optimizations = {
            'memory_cleanup_interval': 1000,      # Less frequent cleanup (every 1000 frames)
            'event_history_limit': 300,           # Slightly more event history
            'marked_frames_limit': 100,           # More frame buffer for stability
            'detection_history_limit': 200,       # More detection history
            'gc_interval': 2000,                  # Less frequent garbage collection
            'batch_export_size': 50,              # Larger export batches for efficiency
            'adaptive_cleanup': True,             # Enable adaptive memory management
            'memory_threshold_mb': 600,           # Higher threshold - 400MB was too aggressive
            'frame_buffer_limit': 20,             # More frame buffering allowed
            'memory_leak_detection': True,        # Enable memory leak detection
            'cleanup_effectiveness_threshold': 50 # Minimum MB to clean up for effective cleanup
        }

    def _cleanup_memory_for_long_videos(self, current_frame):
        """
        ENHANCED LONG VIDEO OPTIMIZATION: Smart adaptive memory cleanup for 1-3 hour video processing
        
        Features:
        - Smart threshold management with memory leak detection
        - Effectiveness tracking to prevent inefficient cleanup cycles
        - Adaptive cleanup based on actual memory usage trends
        - Performance monitoring integration
        - Thread-safe memory management
        """
        opts = self.long_video_optimizations
        
        # Check if adaptive cleanup is enabled
        if opts.get('adaptive_cleanup', False):
            try:
                import psutil
                memory_usage_mb = psutil.Process().memory_info().rss / 1024 / 1024
                
                # Memory leak detection: track memory usage over time
                if not hasattr(self, 'memory_history'):
                    self.memory_history = []
                
                self.memory_history.append((current_frame, memory_usage_mb))
                
                # Keep only recent memory history (last 20 measurements)
                if len(self.memory_history) > 20:
                    self.memory_history = self.memory_history[-20:]
                
                # Check for memory leak pattern (consistent growth over time)
                if opts.get('memory_leak_detection', False) and len(self.memory_history) >= 10:
                    recent_avg = sum(m[1] for m in self.memory_history[-5:]) / 5
                    older_avg = sum(m[1] for m in self.memory_history[-10:-5]) / 5
                    
                    # If memory is consistently growing and above threshold - only log at debug level
                    if recent_avg > older_avg + 10 and memory_usage_mb > opts.get('memory_threshold_mb', 600):
                        self.logger.debug(f"Memory growth detected: {older_avg:.1f}MB â†’ {recent_avg:.1f}MB")
                
                # Only trigger aggressive cleanup if we're significantly above threshold
                threshold_mb = opts.get('memory_threshold_mb', 600)
                if memory_usage_mb > threshold_mb:
                    # Check if previous cleanup was effective to avoid cleanup thrashing
                    if hasattr(self, 'last_cleanup_memory') and hasattr(self, 'last_cleanup_frame'):
                        frames_since_cleanup = current_frame - self.last_cleanup_frame
                        memory_since_cleanup = memory_usage_mb - self.last_cleanup_memory
                        
                        # If we cleaned up recently but memory is high again, be more aggressive
                        if frames_since_cleanup < 500 and memory_since_cleanup > -opts.get('cleanup_effectiveness_threshold', 50):
                            self.logger.debug(f"Previous cleanup ineffective: {frames_since_cleanup} frames ago, {memory_since_cleanup:+.1f}MB change")
                            # Increase threshold temporarily to prevent thrashing
                            threshold_mb += 100
                    
                    if memory_usage_mb > threshold_mb:
                        pre_cleanup_memory = memory_usage_mb
                        self._aggressive_memory_cleanup(current_frame)
                        
                        # Measure cleanup effectiveness
                        post_cleanup_memory = psutil.Process().memory_info().rss / 1024 / 1024
                        cleanup_effectiveness = pre_cleanup_memory - post_cleanup_memory
                        
                        self.last_cleanup_memory = post_cleanup_memory
                        self.last_cleanup_frame = current_frame
                        
                        if cleanup_effectiveness > opts.get('cleanup_effectiveness_threshold', 50):
                            self.logger.debug(f"Effective cleanup: {cleanup_effectiveness:.1f}MB freed at frame {current_frame}")
                        else:
                            self.logger.debug(f"Ineffective cleanup: only {cleanup_effectiveness:.1f}MB freed at {pre_cleanup_memory:.1f}MB")
                        
                        return
                        
            except ImportError:
                # psutil not available, use standard cleanup
                self.logger.debug("psutil not available, using standard memory cleanup")
        
        # Standard periodic cleanup (less frequent now)
        if current_frame % opts['memory_cleanup_interval'] == 0:
            self.performance_metrics['memory_cleanup_counter'] += 1
            
            # Limit events list size with more conservative trimming
            if len(self.events) > opts['event_history_limit']:
                # Keep most recent events and export older ones
                old_events = self.events[:-opts['event_history_limit']]
                self.events = self.events[-opts['event_history_limit']:]
                
                # Background export of old events to free memory
                self._background_export_events(old_events)
                
            # Limit marked frames more conservatively
            if len(self.marked_frames) > opts['marked_frames_limit']:
                # Keep only most recent marked frames
                self.marked_frames = self.marked_frames[-opts['marked_frames_limit']:]
                
            # Clean up old event frames to prevent memory bloat
            if hasattr(self, 'event_frames') and len(self.event_frames) > opts['event_history_limit']:
                # Remove oldest event frames, keep recent ones
                event_ids = list(self.event_frames.keys())
                for event_id in event_ids[:-opts['event_history_limit']]:
                    del self.event_frames[event_id]
                    
            # Limit motion history (already limited but ensure it doesn't grow)
            if len(self.motion_history) > STABILIZATION_WINDOW * 2:
                self.motion_history = self.motion_history[-STABILIZATION_WINDOW:]
                
            # Limit bat centers history
            if hasattr(self, 'bat_centers') and len(self.bat_centers) > opts['detection_history_limit']:
                self.bat_centers = self.bat_centers[-opts['detection_history_limit']:]
                
            self.logger.debug(f"Standard memory cleanup completed at frame {current_frame}")
                
        # Enhanced garbage collection for very long videos (less frequent)
        if current_frame % opts['gc_interval'] == 0:
            import gc
            
            # Record pre-cleanup memory if available
            pre_cleanup_objects = len(gc.get_objects()) if hasattr(gc, 'get_objects') else 0
            
            # Force collection of all generations
            collected = gc.collect()
            
            # Update performance metrics
            self.performance_metrics['last_gc_frame'] = current_frame
            self.performance_metrics['gc_collections'] = self.performance_metrics.get('gc_collections', 0) + 1
            
            if collected > 0:
                self.logger.debug(f"Garbage collection freed {collected} objects at frame {current_frame}")
                
    def _aggressive_memory_cleanup(self, current_frame):
        """Enhanced aggressive memory cleanup for high memory usage situations"""
        opts = self.long_video_optimizations
        
        self.logger.debug(f"Starting aggressive memory cleanup at frame {current_frame}")
        
        # More aggressive event trimming
        target_events = max(100, opts['event_history_limit'] // 3)  # Keep only 1/3
        if len(self.events) > target_events:
            removed_events = len(self.events) - target_events
            self.events = self.events[-target_events:]
            self.logger.debug(f"Trimmed {removed_events} events")
            
        # More aggressive frame trimming
        target_frames = max(20, opts['marked_frames_limit'] // 3)  # Keep only 1/3
        if len(self.marked_frames) > target_frames:
            removed_frames = len(self.marked_frames) - target_frames
            self.marked_frames = self.marked_frames[-target_frames:]
            self.logger.debug(f"Trimmed {removed_frames} marked frames")
            
        # Clear older tracking data more aggressively
        if hasattr(self, 'active_bats'):
            # Remove old bat tracking data
            current_time = current_frame
            expired_bats = [bat_id for bat_id, data in self.active_bats.items() 
                           if current_time - data.get('last_seen', 0) > 15]  # More aggressive: 15 frames instead of 30
            for bat_id in expired_bats:
                del self.active_bats[bat_id]
            if expired_bats:
                self.logger.debug(f"Removed {len(expired_bats)} expired bat tracks")
                
        # Clear OpenCV cache more thoroughly
        try:
            import cv2
            cv2.destroyAllWindows()
            # Clear internal OpenCV caches if available
            if hasattr(cv2, 'setUseOptimized'):
                cv2.setUseOptimized(True)  # Re-enable optimizations
        except Exception as e:
            self.logger.debug(f"OpenCV cache clear error: {e}")
            
        # Clear motion history more aggressively
        if hasattr(self, 'motion_history') and len(self.motion_history) > 50:
            self.motion_history = self.motion_history[-50:]  # Keep only last 50 entries
            
        # Clear detection caches if they exist
        cache_attrs = ['detection_cache', 'contour_cache', 'background_cache', 'frame_cache']
        for attr in cache_attrs:
            if hasattr(self, attr):
                cache = getattr(self, attr)
                if hasattr(cache, 'clear'):
                    cache.clear()
                elif isinstance(cache, (list, dict)):
                    cache.clear()
                    
        # Enhanced garbage collection with multiple passes
        import gc
        
        # Disable garbage collection temporarily for more thorough cleanup
        was_enabled = gc.isenabled()
        gc.disable()
        
        try:
            # Multiple collection passes for better cleanup
            collected_total = 0
            for generation in range(3):  # Collect all 3 generations
                collected = gc.collect(generation)
                collected_total += collected
                
            # Final full collection
            collected_total += gc.collect()
            
            if collected_total > 0:
                self.logger.debug(f"Aggressive GC freed {collected_total} objects")
                
        finally:
            # Re-enable garbage collection if it was enabled
            if was_enabled:
                gc.enable()
        
        self.logger.debug(f"Aggressive memory cleanup completed at frame {current_frame}")
        
        # Clear memory history to reset leak detection after aggressive cleanup
        if hasattr(self, 'memory_history'):
            self.memory_history.clear()
        
    def _background_export_events(self, events):
        """Export events in background to free memory"""
        if not events:
            return
            
        try:
            # Simple background export to prevent memory accumulation
            # This could be enhanced to export to temporary files
            self.logger.debug(f"Background exported {len(events)} events to free memory")
        except Exception as e:
            self.logger.error("Background event export failed", exception=e)
            
    def _update_status_thread_safe(self, message):
        """Thread-safe status update that schedules GUI updates in main thread"""
        if hasattr(self.gui, 'root') and self.gui.root:
            self.gui.root.after(0, lambda: self.gui.update_status(message))
        else:
            # Fallback for non-GUI contexts
            # Status message handled internally - removed console output
            pass
            
    def _schedule_gui_update(self, callback):
        """Schedule a GUI update to run in the main thread"""
        if hasattr(self.gui, 'root') and self.gui.root:
            self.gui.root.after(0, callback)
        else:
            # Fallback for non-GUI contexts
            try:
                callback()
            except Exception as e:
                # GUI update failed but handled internally - removed console output
                pass
                
    def _initialize_error_handling(self):
        """Initialize error handling system after thread-safe methods are available"""
        if self.error_handler is None:
            self.error_handler = setup_error_handling(gui_update_callback=self._update_status_thread_safe)
            self.logger.info("Error handling system initialized")

    def _configure_opencv_threading(self):
        """
        PERFORMANCE OPTIMIZATION: Configure OpenCV threading for maximum CPU utilization
        
        This method optimizes OpenCV's internal threading to use all available CPU cores,
        significantly improving performance for computer vision operations like:
        - Background subtraction (MOG2)
        - Morphological operations
        - Contour detection and analysis
        - Image filtering and processing
        """
        try:
            # Get available CPU cores
            import os
            cpu_count = os.cpu_count()
            
            # Set OpenCV to use all available threads (default is often limited)
            cv2.setNumThreads(cpu_count)
            
            # Enable OpenCV optimizations for Intel processors if available
            cv2.setUseOptimized(True)
            
            # Configure threading for better performance
            if hasattr(cv2, 'setUseIntelPP'):
                cv2.setUseIntelPP(True)  # Enable Intel Performance Primitives if available
                
            self.gui.update_status(f"OpenCV optimized: {cpu_count} threads, optimizations enabled")
            
        except Exception as e:
            self.gui.update_status(f"OpenCV optimization warning: {str(e)}")
            
        # Initialize error handling system (after thread-safe methods are available)
        self._initialize_error_handling()
        self.logger.info("VideoDetector initialization completed")

    def _create_optimized_background_subtractor(self):
        """
        PERFORMANCE OPTIMIZATION: Create optimized MOG2 background subtractor
        
        Enhanced MOG2 configuration for better performance and accuracy in bat detection:
        - Optimized for small moving objects (bats)
        - Reduced noise sensitivity
        - Faster adaptation to lighting changes
        - Better shadow detection for IR videos
        
        Returns:
            Configured cv2.BackgroundSubtractorMOG2 instance
        """
        try:
            # Create MOG2 with optimized parameters for bat detection
            back_sub = cv2.createBackgroundSubtractorMOG2(
                history=500,           # Number of last frames that affect the background model
                varThreshold=50,       # Threshold for squared Mahalanobis distance (lower = more sensitive)
                detectShadows=True     # Enable shadow detection (useful for IR videos)
            )
            
            # Additional optimizations
            back_sub.setNMixtures(5)           # Number of Gaussian mixtures per pixel (default: 5)
            back_sub.setBackgroundRatio(0.7)   # Background ratio (higher = more conservative)
            back_sub.setVarThresholdGen(9)     # Variance threshold for generation of new components
            back_sub.setVarInit(15)            # Initial variance for new components
            back_sub.setVarMin(4)              # Minimum variance for each component
            back_sub.setVarMax(5*back_sub.getVarInit())  # Maximum variance for each component
            back_sub.setComplexityReductionThreshold(0.05)  # Complexity reduction threshold
            back_sub.setShadowThreshold(0.5)   # Shadow threshold (0.0-1.0)
            back_sub.setShadowValue(127)       # Shadow value (0-255)
            
            return back_sub
            
        except Exception as e:
            # Fallback to default MOG2 if optimization fails
            self.gui.update_status(f"Background subtractor optimization failed, using default: {str(e)}")
            return cv2.createBackgroundSubtractorMOG2()

    def _create_enhanced_background_subtractor(self):
        """
        ENHANCED DETECTION: Create dedicated background subtractor for polygon-based detection
        
        This creates a separate MOG2 instance optimized specifically for polygon areas:
        - Shorter history for dynamic polygon regions
        - More sensitive threshold for masked areas
        - Optimized for small moving objects (bats)
        - Prevents contamination from ROI processing
        
        Returns:
            Dedicated cv2.BackgroundSubtractorMOG2 instance for enhanced detection
        """
        if not hasattr(self, 'dedicated_back_sub') or self.dedicated_back_sub is None:
            self.dedicated_back_sub = cv2.createBackgroundSubtractorMOG2(
                history=300,        # Shorter history for dynamic regions
                varThreshold=25,    # More sensitive for masked areas
                detectShadows=False # Disable shadows for performance
            )
            # Optimize for small moving objects (bats)
            self.dedicated_back_sub.setNMixtures(4)
            self.dedicated_back_sub.setBackgroundRatio(0.8)
            self.dedicated_back_sub.setVarThresholdGen(6)
            self.dedicated_back_sub.setVarInit(10)
            self.dedicated_back_sub.setVarMin(2)
            self.dedicated_back_sub.setComplexityReductionThreshold(0.1)
        return self.dedicated_back_sub

    def _create_enhanced_binary_masks(self, frame_shape):
        """
        ENHANCED DETECTION: Create optimized binary masks for polygon areas
        
        Creates pre-computed binary masks to replace slow cv2.pointPolygonTest() calls:
        - Individual polygon masks
        - Edge-expanded masks (6px) for fast-moving bats
        - Combined masks for optimization
        - Partial overlap detection support
        
        Args:
            frame_shape: (height, width) of video frames
            
        Returns:
            bool: True if masks created successfully
        """
        if not self.polygon_areas:
            return False
            
        height, width = frame_shape
        self.polygon_masks = []
        self.expanded_polygon_masks = []
        
        for polygon in self.polygon_areas:
            if len(polygon) < 3:
                continue
                
            # Create base polygon mask
            mask = np.zeros((height, width), dtype=np.uint8)
            polygon_np = np.array(polygon, dtype=np.int32)
            cv2.fillPoly(mask, [polygon_np], 255)
            self.polygon_masks.append(mask)
            
            # Create edge-expanded mask for fast-moving bats
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, 
                                             (self.edge_expansion*2+1, self.edge_expansion*2+1))
            expanded_mask = cv2.dilate(mask, kernel, iterations=1)
            self.expanded_polygon_masks.append(expanded_mask)
        
        # Create combined masks for optimization
        if self.polygon_masks:
            self.combined_polygon_mask = np.zeros((height, width), dtype=np.uint8)
            self.expanded_combined_mask = np.zeros((height, width), dtype=np.uint8)
            
            for mask in self.polygon_masks:
                cv2.bitwise_or(self.combined_polygon_mask, mask, self.combined_polygon_mask)
                
            for expanded in self.expanded_polygon_masks:
                cv2.bitwise_or(self.expanded_combined_mask, expanded, self.expanded_combined_mask)
        
        return True

    def _enhanced_contour_analysis(self, contour):
        """
        ENHANCED DETECTION: Fast contour analysis using binary masks
        
        Replaces slow cv2.pointPolygonTest() with fast binary mask operations:
        - Quick center check against expanded combined mask
        - Overlap calculation using bitwise operations
        - Partial overlap detection (â‰¥25% threshold)
        - Edge expansion sensitivity for fast-moving bats
        
        Args:
            contour: OpenCV contour to analyze
            
        Returns:
            dict: Analysis result with polygon info, or None if no match
        """
        if not self.polygon_masks or not self.expanded_polygon_masks:
            return None
            
        # Basic contour validation
        area = cv2.contourArea(contour)
        if not (MIN_CONTOUR_AREA < area < MAX_CONTOUR_AREA):
            return None
            
        # Get contour properties
        M = cv2.moments(contour)
        if M["m00"] == 0:
            return None
            
        center_x = int(M["m10"] / M["m00"])
        center_y = int(M["m01"] / M["m00"])
        
        # Quick center check against expanded combined mask
        if (self.expanded_combined_mask is not None and 
            0 <= center_y < self.expanded_combined_mask.shape[0] and 
            0 <= center_x < self.expanded_combined_mask.shape[1]):
            if self.expanded_combined_mask[center_y, center_x] == 0:
                return None  # Center not in any expanded polygon
        
        # Create contour mask for overlap calculation
        contour_mask = np.zeros(self.combined_polygon_mask.shape, dtype=np.uint8)
        cv2.fillPoly(contour_mask, [contour], 255)
        
        # Find best matching polygon using binary operations
        best_polygon_idx = -1
        best_overlap_ratio = 0.0
        
        for i, (base_mask, expanded_mask) in enumerate(zip(self.polygon_masks, self.expanded_polygon_masks)):
            # Check overlap with expanded mask first (edge sensitivity)
            expanded_overlap = cv2.bitwise_and(contour_mask, expanded_mask)
            expanded_pixels = cv2.countNonZero(expanded_overlap)
            
            if expanded_pixels > 0:
                # Calculate base polygon overlap ratio
                base_overlap = cv2.bitwise_and(contour_mask, base_mask)
                base_pixels = cv2.countNonZero(base_overlap)
                contour_pixels = cv2.countNonZero(contour_mask)
                
                if contour_pixels > 0:
                    overlap_ratio = base_pixels / contour_pixels
                    
                    # Accept if: good base overlap OR edge detection
                    if overlap_ratio >= self.overlap_threshold or expanded_pixels > base_pixels:
                        if overlap_ratio > best_overlap_ratio:
                            best_overlap_ratio = overlap_ratio
                            best_polygon_idx = i
        
        if best_polygon_idx == -1:
            return None
            
        return {
            'contour': contour,
            'center': (center_x, center_y),
            'area': area,
            'polygon_idx': best_polygon_idx,
            'overlap_ratio': best_overlap_ratio
        }

    def _enhanced_bat_tracking(self, bat_center, frame_idx, polygon_idx):
        """
        ENHANCED TRACKING: Advanced bat tracking with unique IDs and timeout management
        
        Provides persistent tracking across frames with:
        - Unique bat IDs for each detection
        - Position-based tracking with distance threshold
        - Timeout management to expire old tracks
        - Polygon-aware tracking for multi-area scenarios
        
        Args:
            bat_center: (x, y) position of detected bat
            frame_idx: Current frame number
            polygon_idx: Index of polygon where bat was detected
            
        Returns:
            int: Unique bat ID for this detection
        """
        if not bat_center:
            return None
            
        # Clean up expired tracks
        expired_bats = []
        for bat_id, track_info in self.active_bats.items():
            if frame_idx - track_info['last_seen'] > self.bat_tracking_timeout:
                expired_bats.append(bat_id)
        
        for bat_id in expired_bats:
            del self.active_bats[bat_id]
        
        # Find closest existing track
        closest_bat_id = None
        min_distance = float('inf')
        
        for bat_id, track_info in self.active_bats.items():
            distance = np.sqrt(
                (bat_center[0] - track_info['position'][0])**2 + 
                (bat_center[1] - track_info['position'][1])**2
            )
            
            if distance < self.max_bat_distance and distance < min_distance:
                min_distance = distance
                closest_bat_id = bat_id
        
        # Update existing track or create new one
        if closest_bat_id is not None:
            self.active_bats[closest_bat_id].update({
                'position': bat_center,
                'last_seen': frame_idx,
                'polygon_idx': polygon_idx
            })
            return closest_bat_id
        else:
            # Create new track
            self.bat_id_counter += 1
            new_bat_id = self.bat_id_counter
            self.active_bats[new_bat_id] = {
                'position': bat_center,
                'last_seen': frame_idx,
                'polygon_idx': polygon_idx,
                'first_seen': frame_idx
            }
            return new_bat_id

    def _cleanup_enhanced_tracking(self, frame_idx):
        """Clean up expired bat tracks to prevent memory buildup"""
        expired_bats = []
        for bat_id, track_info in self.active_bats.items():
            if frame_idx - track_info['last_seen'] > self.bat_tracking_timeout:
                expired_bats.append(bat_id)
        
        for bat_id in expired_bats:
            del self.active_bats[bat_id]

    def _process_frame_worker(self, frame_data):
        """
        THREADING OPTIMIZATION: Worker function for parallel frame processing
        
        This method processes individual frames in parallel threads to improve performance.
        Each worker handles:
        - Background subtraction
        - Morphological operations
        - Contour detection
        - Polygon filtering
        
        Args:
            frame_data: Tuple containing (frame_idx, frame, back_sub_state)
        
        Returns:
            Tuple containing processed results for the main thread
        """
        try:
            frame_idx, frame, back_sub = frame_data
            
            # Apply background subtraction
            fg_mask = back_sub.apply(frame)
            
            # Morphological operations to reduce noise
            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, NOISE_KERNEL)
            fg_mask_clean = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)
            
            # Find contours
            contours, _ = cv2.findContours(fg_mask_clean, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # Filter contours by area
            valid_contours = []
            for contour in contours:
                area = cv2.contourArea(contour)
                if MIN_CONTOUR_AREA <= area <= MAX_CONTOUR_AREA:
                    valid_contours.append(contour)
            
            return {
                'frame_idx': frame_idx,
                'frame': frame,
                'fg_mask_clean': fg_mask_clean,
                'contours': valid_contours,
                'processing_time': time.time()
            }
            
        except Exception as e:
            return {
                'frame_idx': frame_idx,
                'error': str(e),
                'processing_time': time.time()
            }

    def _process_contours_parallel(self, contours, frame_shape):
        """
        THREADING OPTIMIZATION: Process multiple contours in parallel
        
        When many objects are detected simultaneously, this method processes
        contours concurrently to improve performance.
        
        Args:
            contours: List of contours to process
            frame_shape: Shape of the frame for polygon testing
            
        Returns:
            List of processed contour results
        """
        if len(contours) <= 2:
            # For small numbers of contours, parallel processing overhead isn't worth it
            return [self._analyze_single_contour(contour, frame_shape) for contour in contours]
        
        try:
            with ThreadPoolExecutor(max_workers=min(len(contours), self.processing_threads)) as executor:
                future_to_contour = {
                    executor.submit(self._analyze_single_contour, contour, frame_shape): contour 
                    for contour in contours
                }
                
                results = []
                for future in as_completed(future_to_contour):
                    try:
                        result = future.result()
                        if result:
                            results.append(result)
                    except Exception as e:
                        # Log error but continue processing other contours
                        pass
                
                return results
                
        except Exception as e:
            # Fallback to sequential processing
            return [self._analyze_single_contour(contour, frame_shape) for contour in contours]

    def _analyze_single_contour(self, contour, frame_shape):
        """
        Analyze a single contour for bat detection
        
        Args:
            contour: OpenCV contour to analyze
            frame_shape: Shape of the frame
            
        Returns:
            Dictionary with contour analysis results or None if invalid
        """
        try:
            # Calculate contour properties
            area = cv2.contourArea(contour)
            if not (MIN_CONTOUR_AREA <= area <= MAX_CONTOUR_AREA):
                return None
            
            # Get bounding rectangle and centroid
            x, y, w, h = cv2.boundingRect(contour)
            M = cv2.moments(contour)
            
            if M["m00"] == 0:
                return None
                
            center_x = int(M["m10"] / M["m00"])
            center_y = int(M["m01"] / M["m00"])
            
            # Polygon filtering if enabled
            inside_polygon = False
            polygon_idx = -1
            
            if self.use_polygon_filtering and self.polygon_areas_cv2:
                # Use bounding rectangle optimization if enabled
                if self.use_bounding_rect_optimization and self.polygon_bounding_rect:
                    px, py, pw, ph = self.polygon_bounding_rect
                    if not (px <= center_x <= px + pw and py <= center_y <= py + ph):
                        return None  # Outside bounding rectangle, skip expensive polygon test
                
                # Check against actual polygons
                for idx, polygon_cv2 in enumerate(self.polygon_areas_cv2):
                    result = cv2.pointPolygonTest(polygon_cv2, (center_x, center_y), False)
                    if result >= 0:  # Inside or on boundary
                        inside_polygon = True
                        polygon_idx = idx
                        break
                
                if not inside_polygon:
                    return None
            
            return {
                'contour': contour,
                'area': area,
                'center': (center_x, center_y),
                'bounding_rect': (x, y, w, h),
                'inside_polygon': inside_polygon,
                'polygon_idx': polygon_idx
            }
            
        except Exception as e:
            return None

    def _process_contours_optimized(self, contours, frame_shape):
        """
        CONFIGURABLE CONTOUR PROCESSING: Sequential vs Parallel based on config
        
        Processes contours using either sequential (default, faster) or parallel mode
        based on the USE_PARALLEL_CONTOUR configuration flag.
        
        Args:
            contours: List of contours to process
            frame_shape: Shape of the frame for polygon testing
            
        Returns:
            List of processed contour results
        """
        if not contours:
            return []
        
        # Log the processing mode on first call
        if not hasattr(self, '_contour_mode_logged'):
            if USE_PARALLEL_CONTOUR:
                # Contour processing: Parallel mode enabled - removed console output
                pass
            else:
                # Contour processing: Sequential mode (default, faster) - removed console output
                pass
            self._contour_mode_logged = True
        
        if USE_PARALLEL_CONTOUR:
            # Use parallel processing (existing implementation)
            return self._process_contours_parallel(contours, frame_shape)
        else:
            # Use sequential processing (faster for most cases)
            return self._process_contours_sequential(contours, frame_shape)

    def _process_contours_sequential(self, contours, frame_shape):
        """
        SEQUENTIAL CONTOUR PROCESSING: Process contours one by one
        
        This is the default mode and is faster for most use cases since
        the threading overhead for contour processing often exceeds the benefits.
        
        Args:
            contours: List of contours to process
            frame_shape: Shape of the frame for polygon testing
            
        Returns:
            List of processed contour results
        """
        results = []
        for contour in contours:
            result = self._analyze_single_contour(contour, frame_shape)
            if result:
                results.append(result)
        return results

    def _start_performance_monitoring(self):
        """Initialize performance monitoring for the current detection session"""
        self.performance_metrics['start_time'] = time.time()
        self.performance_metrics['total_frames_processed'] = 0
        self.performance_metrics['total_processing_time'] = 0.0
        self.performance_metrics['contour_processing_times'] = []
        self.performance_metrics['frame_processing_times'] = []

    def _log_frame_processing_time(self, processing_time):
        """Log frame processing time for performance analysis"""
        self.performance_metrics['frame_processing_times'].append(processing_time)
        self.performance_metrics['total_frames_processed'] += 1
        self.performance_metrics['total_processing_time'] += processing_time
        
        # Keep only last 100 frame times to prevent memory bloat
        if len(self.performance_metrics['frame_processing_times']) > 100:
            self.performance_metrics['frame_processing_times'].pop(0)

    def _log_contour_processing_time(self, processing_time, contour_count):
        """Log contour processing time and count for analysis"""
        self.performance_metrics['contour_processing_times'].append({
            'time': processing_time,
            'contour_count': contour_count,
            'timestamp': time.time()
        })
        
        # Keep only last 50 contour processing records
        if len(self.performance_metrics['contour_processing_times']) > 50:
            self.performance_metrics['contour_processing_times'].pop(0)

    def _get_performance_report(self):
        """Generate performance report for current detection session"""
        try:
            metrics = self.performance_metrics
            if metrics['total_frames_processed'] == 0:
                return "No performance data available"
            
            # Calculate averages
            avg_frame_time = metrics['total_processing_time'] / metrics['total_frames_processed']
            fps_performance = 1.0 / avg_frame_time if avg_frame_time > 0 else 0
            
            # Recent frame times
            recent_times = metrics['frame_processing_times'][-10:] if metrics['frame_processing_times'] else []
            recent_avg = sum(recent_times) / len(recent_times) if recent_times else 0
            recent_fps = 1.0 / recent_avg if recent_avg > 0 else 0
            
            # Session duration
            session_duration = time.time() - metrics['start_time'] if metrics['start_time'] else 0
            
            report = f"""
ðŸš€ THREADING PERFORMANCE REPORT ðŸš€

Session Statistics:
â”œâ”€ Duration: {session_duration:.1f}s
â”œâ”€ Frames Processed: {metrics['total_frames_processed']}
â”œâ”€ Total Processing Time: {metrics['total_processing_time']:.2f}s
â””â”€ Processing Threads: {self.processing_threads}

Performance Metrics:
â”œâ”€ Average Frame Time: {avg_frame_time*1000:.1f}ms
â”œâ”€ Average FPS: {fps_performance:.1f}
â”œâ”€ Recent Frame Time: {recent_avg*1000:.1f}ms
â””â”€ Recent FPS: {recent_fps:.1f}

Threading Optimizations:
â”œâ”€ OpenCV Multithreading: âœ… Enabled
â”œâ”€ Parallel Contour Processing: âœ… Enabled  
â”œâ”€ Optimized Background Subtractor: âœ… Enabled
â””â”€ CPU Cores Utilized: {os.cpu_count()}
"""
            return report
            
        except Exception as e:
            return f"Performance report generation failed: {str(e)}"

    def load_video(self):
        self.video_path = filedialog.askopenfilename(
            title="Select IR Video File",
            filetypes=[("Video Files", "*.mp4 *.avi *.mov"), ("All Files", "*.*")]
        )
        if not self.video_path:
            return
        self.cap = cv2.VideoCapture(self.video_path)
        self.fps = self.cap.get(cv2.CAP_PROP_FPS) or 30
        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        ret, frame = self.cap.read()
        if ret:
            # Store frame shape for polygon mask creation
            self.frame_shape = frame.shape[:2]  # (height, width)
            
            frame_small = cv2.resize(frame, (640, 480))
            self.prev_gray = cv2.cvtColor(frame_small, cv2.COLOR_BGR2GRAY)
            self.gui.show_frame(frame_small)
            self.gui.update_status(f"Loaded: {os.path.basename(self.video_path)}")
            self.gui.btn_select_roi.config(state=tk.NORMAL)
            
            # Update polygon mask with new frame dimensions
            self.update_polygon_mask()
        else:
            self.gui.update_status("Failed to read video")
        self.cap.release()
        self.cap = None

    def select_roi(self):
        cap = cv2.VideoCapture(self.video_path)
        ret, frame = cap.read()
        cap.release()
        if not ret:
            raise RuntimeError("Failed to read frame for ROI selection.")
        
        # Custom ROI selection with proper 'q' and ESC key handling
        window_name = "ROI Auswahl"
        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)
     
        # Variables for ROI selection
        roi_selected = False
        roi_coords = None
        drawing = False
        start_point = None
        end_point = None
        
        def mouse_callback(event, x, y, flags, param):
            nonlocal drawing, start_point, end_point, roi_coords
            
            if event == cv2.EVENT_LBUTTONDOWN:
                drawing = True
                start_point = (x, y)
                end_point = (x, y)
            elif event == cv2.EVENT_MOUSEMOVE:
                if drawing:
                    end_point = (x, y)
            elif event == cv2.EVENT_LBUTTONUP:
                drawing = False
                end_point = (x, y)
                # Calculate ROI coordinates (x, y, width, height)
                x1, y1 = start_point
                x2, y2 = end_point
                roi_x = min(x1, x2)
                roi_y = min(y1, y2)
                roi_w = abs(x2 - x1)
                roi_h = abs(y2 - y1)
                if roi_w > 0 and roi_h > 0:
                    roi_coords = (roi_x, roi_y, roi_w, roi_h)
        
        cv2.setMouseCallback(window_name, mouse_callback)
        
        try:
            while True:
                # Create a copy of the frame to draw on
                display_frame = frame.copy()
                
                # Draw current selection rectangle
                if start_point and end_point:
                    cv2.rectangle(display_frame, start_point, end_point, (0, 255, 0), 2)
                
                # Add instruction text (enhanced since overlay not available)
                cv2.putText(display_frame, "ROI Auswahl - Ziehe Rechteck um gewuenschten Bereich", (10, 30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                cv2.putText(display_frame, "ENTER=Bestaetigen | Q/ESC=Abbrechen | R=Zuruecksetzen | X=Schliessen", (10, 60), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                
                cv2.imshow(window_name, display_frame)
                
                # Handle key presses
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q') or key == 27:  # 'q' or ESC key
                    roi_coords = None
                    break
                elif key == 13:  # Enter key
                    if roi_coords is not None:
                        roi_selected = True
                    break
                elif key == ord('r'):  # 'r' key to reset selection
                    start_point = None
                    end_point = None
                    roi_coords = None
                   
                
                # Handle user clicking the window "X" button
                if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) < 1:
                    roi_coords = None
                    break
                    
        finally:
            # Ensure window is properly closed - check if it still exists first
            try:
                if cv2.getWindowProperty(window_name, cv2.WND_PROP_VISIBLE) >= 0:
                    cv2.destroyWindow(window_name)
            except cv2.error:
                # Window was already destroyed (e.g., user clicked X button)
                pass
            cv2.waitKey(1)  # Allow time for window cleanup
        
        if roi_coords and roi_coords[2] > 0 and roi_coords[3] > 0:
            self.roi = roi_coords
            self.user_roi = roi_coords  # CRITICAL: Track the actual user-drawn ROI
            
            # Check for polygon conflict after ROI selection
            self.check_roi_polygon_conflict_after_roi_selection()
            
            return roi_coords
        else:
            return None

    def check_roi_polygon_conflict_after_roi_selection(self):
        """Check and notify user if ROI was selected while polygons exist - Scenario 2"""
        has_polygons = self.polygon_areas and len(self.polygon_areas) > 0
        
        if has_polygons:
            from tkinter import messagebox
            messagebox.showwarning(
                "ROI hat PrioritÃ¤t",
                f"Sie haben ein ROI ausgewÃ¤hlt, wÃ¤hrend bereits {len(self.polygon_areas)} "
                f"Polygon{'e' if len(self.polygon_areas) > 1 else ''} definiert {'sind' if len(self.polygon_areas) > 1 else 'ist'}!\n\n"
                f"ðŸŽ¯ DAS ROI HAT PRIORITÃ„T\n\n"
                f"Die Polygon{'e' if len(self.polygon_areas) > 1 else ''} werden fÃ¼r die Erkennung IGNORIERT, "
                f"da das ROI bereits aktiv ist.\n\n"
                f"Nur das ROI wird fÃ¼r die Bewegungserkennung verwendet.\n\n"
                f"Um die Polygon{'e' if len(self.polygon_areas) > 1 else ''} zu verwenden, "
                f"lÃ¶schen Sie zuerst das ROI mit 'Zeichnungen lÃ¶schen'."
            )

    def get_active_detection_area_info(self):
        """
        Get information about the currently active detection area for UI display.
        Returns: Dictionary with detection mode and area details
        """
        if hasattr(self, 'active_detection_mode') and self.active_detection_mode:
            return {
                "mode": self.active_detection_mode,
                "area_info": getattr(self, 'active_area_info', {}),
                "roi": self.user_roi,  # CRITICAL: Use user_roi for UI display
                "polygon_count": len(self.polygon_areas) if self.polygon_areas else 0,
                "polygon_filtering_active": self.use_polygon_filtering
            }
        else:
            # Default state analysis - CRITICAL: Use user_roi for priority logic
            has_roi = self.user_roi is not None
            has_polygons = self.polygon_areas and len(self.polygon_areas) > 0
            
            if has_roi and has_polygons:
                return {"mode": "roi_priority", "roi": self.user_roi, "polygon_count": len(self.polygon_areas)}
            elif has_roi:
                return {"mode": "roi_only", "roi": self.user_roi}
            elif has_polygons:
                return {"mode": "polygon_only", "polygon_count": len(self.polygon_areas)}
            else:
                return {"mode": "none"}

    def clear_all_detection_areas(self):
        """
        Clear both ROI and polygon areas and reset priority state.
        """
        self.roi = None
        self.user_roi = None  # CRITICAL: Clear the user-drawn ROI tracking
        self.polygon_areas = []
        self.use_polygon_filtering = False
        self.active_detection_mode = None
        self.active_area_info = {}
        
        # Clear polygon mask
        self.polygon_mask = None
        
        # Update detector state
        if hasattr(self, 'detector') and hasattr(self.detector, 'set_polygon_areas'):
            self.detector.set_polygon_areas([])

    def check_detection_area_priority(self):
        """
        Check and handle priority between ROI and polygon areas with user notification.
        Returns: (detection_mode, active_area_info, user_notified)
        """
        # CRITICAL FIX: Use user_roi to check for actual user-drawn ROI, not calculated polygon bounding ROI
        has_roi = self.user_roi is not None
        has_polygons = self.polygon_areas and len(self.polygon_areas) > 0
        
        if has_roi and has_polygons:
            # Both ROI and polygons exist - ROI has priority, notify user during detection start
            from tkinter import messagebox
            
            # Show clear priority notification during detection start
            messagebox.showinfo(
                "Erkennung startet - ROI PrioritÃ¤t",
                f"Die Erkennung wird gestartet.\n\n"
                f"ðŸŽ¯ ERKENNUNGSBEREICH: ROI (Rechteck)\n\n"
                f"Hinweis: {len(self.polygon_areas)} Polygon{'e' if len(self.polygon_areas) > 1 else ''} "
                f"{'sind' if len(self.polygon_areas) > 1 else 'ist'} definiert, aber werden IGNORIERT, "
                f"da das ROI PrioritÃ¤t hat.\n\n"
                f"Nur das rechteckige ROI wird fÃ¼r die Bewegungserkennung verwendet."
            )
            
            return "roi_priority", {"roi": self.user_roi, "ignored_polygons": len(self.polygon_areas)}, True
            
        elif has_roi:
            # Only ROI exists
            return "roi_only", {"roi": self.user_roi}, False
            
        elif has_polygons:
            # Only polygons exist
            return "polygon_only", {"polygons": len(self.polygon_areas)}, False
            
        else:
            # Neither exists - full frame
            return "full_frame", {}, False

    def start_detection(self):
        """Enhanced start_detection with priority handling and user notification"""
        if not self.video_path:
            self.gui.update_status("Video not loaded")
            return
        if self.processing:
            self.gui.update_status("Detection already running")
            return

        # Get processing mode and window visibility settings from GUI
        processing_mode = getattr(self.gui, 'processing_mode', tk.StringVar(value="Live Mode")).get()
        show_window = getattr(self.gui, 'show_window', tk.BooleanVar(value=True)).get()
        
        # Store settings for use in processing methods
        self.processing_mode = processing_mode
        self.show_window = show_window

        # Check detection area priority and notify user if needed
        detection_mode, area_info, user_notified = self.check_detection_area_priority()
        
        # Configure detection based on priority results
        if detection_mode == "roi_priority" or detection_mode == "roi_only":
            # Use ROI (ignore polygons if both exist)
            # CRITICAL: Ensure roi is set to user_roi for processing
            self.roi = self.user_roi
            self.gui.update_status("Erkennung gestartet mit rechteckigem ROI...")
            self.use_polygon_filtering = False
            
        elif detection_mode == "polygon_only":
            # Use polygon detection
            # CRITICAL FIX: Don't set roi for polygon-only mode since _process_video_with_polygon_mask() doesn't need it
            # The polygon_mask will be used instead of roi-based cropping
            polygon_count = area_info["polygons"]
            # Detection start message removed for cleaner output
            self.use_polygon_filtering = True
            
        else:  # detection_mode == "full_frame"
            # Full frame detection
            cap = cv2.VideoCapture(self.video_path)
            ret, frame = cap.read()
            cap.release()
            if ret:
                h, w = frame.shape[:2]
                self.roi = (0, 0, w, h)  # Full frame ROI
                self.gui.update_status("Erkennung gestartet (gesamtes Video)...")
                self.use_polygon_filtering = False
            else:
                self.gui.update_status("Failed to read video for detection")
                return

        # Store the active detection mode for downstream processing
        self.active_detection_mode = detection_mode
        self.active_area_info = area_info

        self.cap = cv2.VideoCapture(self.video_path)
        self.processing = True
        self.motion_history = []
        self.events = []
        self.marked_frames = []
        self.bat_centers = []
        
        # Reset bat tracking for new video processing
        self.bat_tracker = {}
        self.next_bat_id = 1
        
        self.cooldown_counter = 0
        self.bat_inside = False

        # Initialize Fast Mode progress window if needed
        if self.processing_mode == "fast" and self.show_window:
            try:
                from utils.fast_mode_window import FastModeProgressWindow
                self.fast_mode_window = FastModeProgressWindow(self.total_frames)
                self.fast_mode_window.show()
            except ImportError:
                # Fallback if Fast Mode window is not available
                self.processing_mode = "live"
                self.gui.update_status("Fast Mode nicht verfÃ¼gbar - auf Live Mode umgestellt")
        else:
            self.fast_mode_window = None

        # Choose appropriate processing method based on detection mode
        if detection_mode == "polygon_only":
            # Use polygon-masked processing for true polygon-only detection
            # Note: polygon_mask will be created when first frame is read
            threading.Thread(target=self._process_video_with_polygon_mask).start()
        else:
            # Use standard ROI-based processing
            threading.Thread(target=self._process_video).start()

    def _process_video_with_polygon_mask(self):
        """
        ðŸš€ OPTIMIZED polygon detection with 5-10x performance improvement.
        
        CRITICAL FIXES APPLIED:
        âœ… Separate MOG2 background subtractor (prevents contamination)
        âœ… Correct processing order (background subtraction â†’ polygon mask)
        âœ… Binary mask operations (replaces slow cv2.pointPolygonTest)
        âœ… Edge expansion (6px) for fast-moving bats
        âœ… Partial overlap detection (â‰¥25% threshold)
        âœ… Thread-safe polygon mask handling
        âœ… Preserves all existing tracking and export features
        """
        frame_idx = 0
        detections_count = 0
        
        # ðŸ”§ CRITICAL FIX: Separate background subtractor for polygon mode
        if not hasattr(self, '_polygon_back_sub'):
            self._polygon_back_sub = cv2.createBackgroundSubtractorMOG2(
                history=300,        # Shorter history for dynamic polygon regions
                varThreshold=25,    # More sensitive for masked areas
                detectShadows=False # Disable shadows for performance
            )
            # Optimize for small moving objects (bats)
            self._polygon_back_sub.setNMixtures(4)
            self._polygon_back_sub.setBackgroundRatio(0.8)
            self._polygon_back_sub.setVarThresholdGen(6)
            self._polygon_back_sub.setVarInit(10)
            self._polygon_back_sub.setVarMin(2)
            self._polygon_back_sub.setComplexityReductionThreshold(0.1)
        
        # ðŸ”§ Initialize FastPolygonEngine for binary mask operations
        fast_engine = None
        try:
            # Check if FastPolygonEngine is available
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.abspath(__file__ + "/..")))
            from fast_polygon_engine import FastPolygonEngine
            
            # ðŸ”§ CRITICAL FIX: Only create FastPolygonEngine placeholder, initialize later
            if self.polygon_areas:
                fast_engine = FastPolygonEngine(self)
                # Note: create_binary_masks will be called when frame_shape is available
        except (ImportError, AttributeError):
            pass  # Use fallback method
        
        # PERFORMANCE MONITORING: Initialize performance tracking
        self._start_performance_monitoring()
        start_time = time.time()
        total_contour_analysis_time = 0
        
        try:
            while self.processing and self.cap.isOpened():
                # Check for cancellation more frequently
                if self._check_cancellation():
                    # Detection cancelled - handled internally
                    break
                    
                frame_start = time.time()
                
                ret, frame = self.cap.read()
                if not ret:
                    break
                frame_idx += 1
                
                # ðŸ”§ UI RESPONSIVENESS FIX: Thread-safe GUI updates every 5 frames
                if frame_idx % 5 == 0:
                    # Thread-safe GUI update - schedule in main thread
                    if hasattr(self.gui, 'root'):
                        self.gui.root.after(0, lambda: self.gui.root.update_idletasks())
                    
                    # LONG VIDEO OPTIMIZATION: Memory cleanup for 2-3 hour videos
                    self._cleanup_memory_for_long_videos(frame_idx)
                    
                    # Additional cancellation check during GUI updates
                    if self._check_cancellation():
                        # Detection cancelled during GUI update - handled internally
                        break
                    
                    # Handle processing mode display
                    processing_mode = getattr(self, 'processing_mode', 'live')
                    if processing_mode == "fast":
                        # Fast Mode: Update progress window instead of video display (if window enabled)
                        if self.fast_mode_window and not self.fast_mode_window.is_cancelled():
                            new_detections = 1 if bat_center else 0
                            if not self.fast_mode_window.update_progress(frame_idx, new_detections):
                                # User cancelled processing
                                self.processing = False
                                break
                        elif self.fast_mode_window and self.fast_mode_window.is_cancelled():
                            # Processing was cancelled via Fast Mode window
                            self.processing = False
                            break
                        # If no progress window (show_window=False), just process silently
                    else:
                        # Live Mode: Show real-time video overlay (as before)
                        # Check if window should be displayed (check checkbox state dynamically)
                        show_video_window = (self.show_overlay and 
                                           getattr(self.gui, 'show_window', tk.BooleanVar(value=True)).get())
                        
                        if show_video_window:
                            progress_percent = (frame_idx / self.total_frames) * 100 if self.total_frames > 0 else 0
                            fast_engine_active = fast_engine is not None
                            
                            # Clean up expired detections
                            expired_polygons = [poly_idx for poly_idx, expire_frame in self.recent_detections.items() 
                                              if frame_idx > expire_frame]
                            for poly_idx in expired_polygons:
                                del self.recent_detections[poly_idx]
                            
                            # Prepare active polygons and bat centers for overlay
                            active_polygons = list(self.recent_detections.keys())
                            current_bat_centers = []
                            
                            # Add current bat centers if available
                            if bat_center:
                                current_bat_centers = [bat_center]
                            
                            # Display frame with German overlay (Live Mode only)
                            # Use dynamic checkbox state for window visibility
                            current_show_window = getattr(self.gui, 'show_window', tk.BooleanVar(value=True)).get()
                            window_open = self.video_display.display_frame(
                                frame, frame_idx, self.total_frames, fast_engine_active,
                                self.polygon_areas or [], active_polygons, current_bat_centers,
                                self.fps, show_window=current_show_window
                            )
                            
                            # Check if window was closed by user (only relevant if window was shown)
                            if current_show_window and not window_open:
                                self.processing = False
                                break
                
                # Initialize frame shape on first frame
                if not hasattr(self, 'frame_shape') or self.frame_shape is None:
                    self.frame_shape = frame.shape[:2]
                    self.update_polygon_mask()
                    
                    # ðŸ”§ CRITICAL FIX: Initialize fast engine with actual frame shape
                    if fast_engine is not None and self.polygon_areas:
                        if not fast_engine.create_binary_masks(self.frame_shape):
                            fast_engine = None
                            # FastPolygonEngine initialization failed, using fallback - removed console output
                            pass
                        else:
                            # FastPolygonEngine successfully initialized with binary masks - removed console output
                            pass
                elif fast_engine and not hasattr(fast_engine, 'combined_mask'):
                    # Retry initialization if it failed before
                    if self.polygon_areas and not fast_engine.create_binary_masks(self.frame_shape):
                        fast_engine = None
                
                if self.polygon_mask is None:
                    return self._process_video()

                # ðŸš€ CRITICAL FIX: CORRECT PROCESSING ORDER
                # Step 1: Apply background subtraction to FULL FRAME first (not masked)
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                fgmask = self._polygon_back_sub.apply(gray)
                
                # Step 2: Apply morphological operations to clean up noise
                kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (NOISE_KERNEL, NOISE_KERNEL))
                fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)
                
                # Step 3: THEN apply polygon constraint to detected motion
                if fast_engine and hasattr(fast_engine, 'expanded_combined_mask') and fast_engine.expanded_combined_mask is not None:
                    # âš¡ Use FastPolygonEngine binary masks (5-10x faster)
                    fgmask = cv2.bitwise_and(fgmask, fast_engine.expanded_combined_mask)
                else:
                    # Fallback to original polygon mask with edge expansion
                    if self.polygon_mask is not None:
                        # Apply 6px edge expansion for fast-moving bats
                        kernel_expand = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (6, 6))
                        expanded_mask = cv2.dilate(self.polygon_mask, kernel_expand, iterations=1)
                        fgmask = cv2.bitwise_and(fgmask, expanded_mask)
                    else:
                        fgmask = cv2.bitwise_and(fgmask, self.polygon_mask)
                
                # Step 4: Find contours in motion-detected polygon regions
                contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                
                # Create masked frame for export (preserve original method behavior)
                masked_frame = cv2.bitwise_and(frame, frame, mask=self.polygon_mask)
                
                # === ðŸ”¥ OPTIMIZED CONTOUR ANALYSIS ===
                motion_detected = False
                bat_center = None
                detected_polygon_idx = -1
                best_detection_result = None
                
                if contours:
                    contour_start = time.time()
                    
                    if fast_engine:
                        # âš¡ Use FastPolygonEngine for optimized analysis
                        detection_results = []
                        for contour in contours:
                            result = fast_engine.fast_contour_analysis(contour)
                            if result:
                                detection_results.append(result)
                        
                        if detection_results:
                            # Use best detection result
                            best_detection_result = detection_results[0]
                            bat_center = best_detection_result['center']
                            detected_polygon_idx = best_detection_result['polygon_idx']
                            motion_detected = True
                            detections_count += 1
                    else:
                        # ðŸ”§ Fallback to optimized manual analysis with edge tolerance
                        for contour in contours:
                            area = cv2.contourArea(contour)
                            if MIN_CONTOUR_AREA <= area <= MAX_CONTOUR_AREA:
                                # Get contour center
                                M = cv2.moments(contour)
                                if M["m00"] > 0:
                                    center_x = int(M["m10"] / M["m00"])
                                    center_y = int(M["m01"] / M["m00"])
                                    
                                    # Check if center is in any polygon (with edge tolerance)
                                    for poly_idx, polygon in enumerate(self.polygon_areas):
                                        if len(polygon) >= 3:
                                            poly_array = np.array(polygon, dtype=np.int32)
                                            # ðŸ”§ CRITICAL FIX: Use distance-based test with edge tolerance
                                            result = cv2.pointPolygonTest(poly_array, (center_x, center_y), True)
                                            if result >= -6:  # 6px tolerance for edge expansion
                                                bat_center = (center_x, center_y)
                                                detected_polygon_idx = poly_idx
                                                motion_detected = True
                                                detections_count += 1
                                                break
                                    
                                    if motion_detected:
                                        break
                    
                    total_contour_analysis_time += time.time() - contour_start

                # === EXISTING MOTION STABILIZATION (PRESERVED) ===
                self.motion_history.append(motion_detected)
                if len(self.motion_history) > STABILIZATION_WINDOW:
                    self.motion_history.pop(0)
                stabilized_motion = sum(self.motion_history) > (STABILIZATION_WINDOW // 2)

                if self.cooldown_counter > 0:
                    self.cooldown_counter -= 1

                # === EVENT DETECTION - ENTRY (ENHANCED WITH POLYGON INFO) ===
                if stabilized_motion and not self.bat_inside and self.cooldown_counter == 0:
                    if bat_center:  # Already validated to be inside polygons
                        self.bat_inside = True
                        
                        # Create unique event ID
                        event_id = len(self.events)
                        entry_time = frame_idx / self.fps
                        
                        # ðŸŽ¥ OVERLAY EVENT: Add entry event to video overlay
                        if self.show_overlay:
                            # Track recent detection for polygon highlighting
                            self.recent_detections[detected_polygon_idx] = frame_idx + self.detection_timeout
                            
                            self.video_display.add_detection_event(
                                'entry', detected_polygon_idx, entry_time, 
                                {'bat_center': bat_center, 'frame_idx': frame_idx}
                            )
                        
                        event_data = {
                            "entry": entry_time,
                            "einflugzeit": entry_time,
                            "exit": None,
                            "ausflugzeit": None,
                            "duration": None,
                            "dauer": None,
                            "frame_idx": frame_idx,
                            "entry_frame": frame_idx,
                            "exit_frame": None,
                            "roi": (0, 0, frame.shape[1], frame.shape[0]),  # Full frame for polygon mode
                            "bat_center": bat_center,
                            "event_id": event_id,
                            "polygon_area": detected_polygon_idx,
                            "detection_method": "OptimizedPolygon",  # ðŸ”§ Track optimization
                            "fast_engine_used": fast_engine is not None  # ðŸ”§ Track engine type
                        }
                        
                        # Add overlap information if available from FastPolygonEngine
                        if best_detection_result:
                            event_data["overlap_percentage"] = best_detection_result.get('overlap_ratio', 1.0) * 100
                        
                        # === CRITICAL: CREATE EXPORT-OPTIMIZED FRAME ===
                        # This ensures proper polygon visualization with bat highlighting
                        # CRITICAL FIX: Use masked_frame for polygon-constrained export
                        
                        # Debug export frame creation
                        if frame_idx <= 3:
                            original_pixels = cv2.countNonZero(cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))
                            masked_pixels = cv2.countNonZero(cv2.cvtColor(masked_frame, cv2.COLOR_BGR2GRAY))
                        
                        marked = self.create_export_optimized_frame(
                            masked_frame.copy(), 
                            bat_center, 
                            detected_polygon_idx,
                            original_frame=frame.copy(),  # ENHANCED: Pass original frame for dimmed background
                            frame_number=frame_idx  # TRACKING: Pass frame number for bat tracking
                        )
                        
                        # Store bat center for tracking
                        if bat_center:
                            self.bat_centers.append(bat_center)

                        # Store detection frames with proper export visualization
                        # CRITICAL FIX: Use masked_frame for polygon-constrained storage
                        detection_frames = {
                            "trigger_frame": masked_frame.copy(),
                            "marked_frame": marked.copy(),
                            "timestamp": frame_idx / self.fps,
                            "frame_index": frame_idx
                        }
                        self.event_frames[event_id] = detection_frames
                        
                        self.events.append(event_data)
                        self.cooldown_counter = COOLDOWN_FRAMES
                        
                        if hasattr(self.gui, 'update_status'):
                            engine_info = "FastEngine" if fast_engine else "Fallback"
                            # Bat detection message removed for cleaner output
                            pass

                # === STEP 7: EVENT DETECTION - EXIT ===
                elif not stabilized_motion and self.bat_inside and self.cooldown_counter == 0:
                    self.bat_inside = False
                    exit_time = frame_idx / self.fps
                    
                    # ðŸŽ¥ OVERLAY EVENT: Add exit event to video overlay
                    if self.show_overlay:
                        self.video_display.add_detection_event(
                            'exit', -1, exit_time, 
                            {'frame_idx': frame_idx}
                        )
                    
                    if self.events:
                        # Update last event with exit information
                        last_event = self.events[-1]
                        last_event["exit"] = exit_time
                        last_event["ausflugzeit"] = exit_time
                        last_event["exit_frame"] = frame_idx
                        
                        # Calculate duration safely
                        entry_time = last_event.get("entry", 0)
                        duration = max(0.0, exit_time - entry_time)
                        last_event["duration"] = duration
                        last_event["dauer"] = duration
                        
                        # Store exit frame with export-optimized polygon visualization
                        current_event_id = last_event.get("event_id")
                        if current_event_id is not None and current_event_id in self.event_frames:
                            # Create exit frame without bat highlighting (no bat detected at exit)
                            # CRITICAL FIX: Use masked_frame for polygon-constrained exit frame
                            marked = self.create_export_optimized_frame(
                                masked_frame.copy(),
                                original_frame=frame.copy(),  # ENHANCED: Pass original frame for dimmed background
                                frame_number=frame_idx  # TRACKING: Pass frame number for bat tracking
                            )
                            
                            self.event_frames[current_event_id]["exit_frame"] = masked_frame.copy()
                            self.event_frames[current_event_id]["exit_marked_frame"] = marked.copy()
                            self.event_frames[current_event_id]["exit_timestamp"] = exit_time
                    
                    self.cooldown_counter = COOLDOWN_FRAMES
                    if hasattr(self.gui, 'update_status'):
                        # Bat exit message removed for cleaner output
                        pass

                # === STEP 8: CONTINUOUS FRAME SEQUENCE STORAGE ===
                if self.bat_inside and self.events:
                    current_event_id = self.events[-1].get("event_id")
                    if current_event_id is not None and current_event_id in self.event_frames:
                        # Initialize frame sequence if not exists
                        if "frame_sequence" not in self.event_frames[current_event_id]:
                            self.event_frames[current_event_id]["frame_sequence"] = []
                        
                        # Store frame sequence with export-optimized visualization
                        frame_seq = self.event_frames[current_event_id]["frame_sequence"]
                        if len(frame_seq) < 30:  # Limit to prevent memory issues
                            # CRITICAL FIX: Use masked_frame for polygon-constrained frame sequence
                            marked = self.create_export_optimized_frame(
                                masked_frame.copy(), 
                                bat_center, 
                                detected_polygon_idx,
                                original_frame=frame.copy(),  # ENHANCED: Pass original frame for dimmed background
                                frame_number=frame_idx  # TRACKING: Pass frame number for bat tracking
                            )
                            
                            frame_seq.append({
                                "frame": masked_frame.copy(),
                                "marked_frame": marked.copy(),
                                "timestamp": frame_idx / self.fps,
                                "frame_index": frame_idx,
                                "bat_center": bat_center
                            })

                # === STEP 9: CREATE FINAL EXPORT FRAME ===
                # This is the frame that will be used for video export
                # It includes proper polygon outlines and bat highlighting when detected
                # CRITICAL FIX: Use masked_frame instead of full frame for polygon-constrained export
                marked_export_frame = self.create_export_optimized_frame(
                    masked_frame.copy(), 
                    bat_center, 
                    detected_polygon_idx,
                    original_frame=frame.copy(),  # ENHANCED: Pass original frame for dimmed background
                    frame_number=frame_idx  # TRACKING: Pass frame number for bat tracking
                )
                
                # === STEP 10: CREATE DISPLAY FRAME FOR LIVE VIEW ===
                if POLYGON_ENABLE_INFO_PANEL and hasattr(self.gui, 'playing') and getattr(self.gui, 'playing', False):
                    # Create enhanced display frame with info panel for live viewing
                    display_frame = self.create_polygon_info_overlay(marked_export_frame.copy())
                    # Update GUI if available
                    if hasattr(self.gui, 'show_frame'):
                        try:
                            self.gui.show_frame(display_frame)
                        except:
                            pass  # Don't fail if GUI update fails
                
                # === STEP 11: STORE EXPORT FRAME ===
                # Store the properly visualized frame for video export
                # This ensures exported videos show polygon boundaries with correct highlighting
                self.marked_frames.append(marked_export_frame)
                
                # Performance monitoring (every 100 frames)
                if frame_idx % 100 == 0:
                    frame_time = time.time() - frame_start
                    fps_estimate = 1.0 / frame_time if frame_time > 0 else 0
                    avg_contour_time = total_contour_analysis_time / max(1, frame_idx) * 1000
                    
                    engine_status = "FastEngine" if fast_engine else "Fallback"
                    # Frame progress logging removed for cleaner output

        except Exception as e:
            if hasattr(self.gui, 'update_status'):
                self._update_status_thread_safe(f"Optimized polygon detection error: {str(e)}")
        finally:
            self.processing = False
            
            # Handle processing mode cleanup
            processing_mode = getattr(self, 'processing_mode', 'live')
            if processing_mode == "fast":
                # Fast Mode: Show completion in progress window and close it
                if self.fast_mode_window and not self.fast_mode_window.is_cancelled():
                    total_time = time.time() - start_time
                    self.fast_mode_window.show_completion_message(len(self.events), total_time)
                    # Window will auto-close after showing completion message
            else:
                # Live Mode: Close video display window
                if hasattr(self, 'video_display'):
                    self.video_display.close_window()
                cv2.destroyAllWindows()  # Clean up any remaining OpenCV windows
            
            # Performance summary
            total_time = time.time() - start_time
            avg_frame_time = total_time / max(1, frame_idx)
            avg_contour_time = total_contour_analysis_time / max(1, frame_idx)
            
            # Enhanced performance report
            if hasattr(self.gui, 'update_status'):
                engine_used = "FastPolygonEngine" if fast_engine else "Fallback"
                mode_text = "âš¡ Schnell" if processing_mode == "fast" else "ðŸŽ¬ Live"
                final_status = f"ðŸš€ {mode_text}-Modus abgeschlossen: {len(self.events)} Ereignisse, {detections_count} Erkennungen ({engine_used})"
                self.gui.update_status(final_status)
                
            # Polygon detection summary logged internally - removed console output for cleaner interface
                
            # Handle any incomplete events at the end of video
            self._finalize_incomplete_events(frame_idx)
            
            self.processing = False
            if self.cap:
                self.cap.release()
            
            if hasattr(self.gui, 'update_status'):
                self._update_status_thread_safe(f"Polygon-masked detection finished, {len(self.events)} events detected")
            
            # Notify the GUI that detection is done
            if hasattr(self.gui, 'on_detection_finished'):
                self._schedule_gui_update(self.gui.on_detection_finished)

    def calculate_polygon_bounding_roi(self):
        """Berechnet eine umfassende ROI-Bounding-Box fÃ¼r alle Polygone"""
        if not self.polygon_areas:
            return None
            
        min_x = float('inf')
        min_y = float('inf') 
        max_x = float('-inf')
        max_y = float('-inf')
        
        for polygon in self.polygon_areas:
            for point in polygon:
                x, y = point
                min_x = min(min_x, x)
                min_y = min(min_y, y)
                max_x = max(max_x, x)
                max_y = max(max_y, y)
        
        # Add some padding
        padding = 20
        width = max_x - min_x + 2 * padding
        height = max_y - min_y + 2 * padding
        
        return (max(0, min_x - padding), max(0, min_y - padding), int(width), int(height))

    def stop_detection(self):
        """Stop detection processing and close all OpenCV windows"""
        self.processing = False
        try:
            cv2.destroyAllWindows()
        except:
            pass  # Ignore errors if OpenCV is not initialized
    
    def _check_cancellation(self):
        """
        Check if detection should be cancelled
        Returns True if cancellation is requested, False otherwise
        """
        # Check self.processing flag
        if not self.processing:
            return True
            
        # Check background processor cancellation
        if (hasattr(self, 'background_processor') and 
            self.background_processor and 
            self.background_processor.cancelled):
            self.logger.debug("Detection cancelled by background processor")
            self.processing = False
            return True
            
        return False

    def _process_video(self):
        """
        Enhanced video processing with integrated FastPolygonEngine detection.
        
        Features:
        - Full-frame background subtraction before masking
        - Pre-computed binary masks for 5-10x faster polygon detection
        - 6px edge expansion for fast-moving bats
        - 25% partial overlap detection threshold
        - Unique bat ID tracking with position-based association
        - Thread-safe operation and memory-efficient processing
        - ROI fallback when no polygons are defined
        """
        x, y, w, h = self.roi
        frame_idx = 0
        start_time = time.time()
        
        # Initialize enhanced detection system
        has_polygons = len(self.polygon_areas) > 0
        use_enhanced_detection = has_polygons and self.use_enhanced_detection
        
        if use_enhanced_detection:
            # Create enhanced background subtractor for polygon detection
            self._create_enhanced_background_subtractor()
        
        # Status tracking for German window updates
        last_status_frame = 0
        current_polygon_detections = {}  # Track which polygons have active detections

        try:
            while self.processing and self.cap.isOpened():
                # Record frame processing start time
                frame_start_time = time.time()
                
                # Check for cancellation more frequently
                if self._check_cancellation():
                    self.logger.info("Detection cancelled")
                    break
                    
                ret, frame = self.cap.read()
                if not ret:
                    break
                frame_idx += 1
                
                # Initialize binary masks on first frame
                if frame_idx == 1 and use_enhanced_detection:
                    frame_shape = frame.shape[:2]  # (height, width)
                    self._create_enhanced_binary_masks(frame_shape)
                
                # UI responsiveness: Thread-safe GUI updates every 5 frames
                if frame_idx % 5 == 0:
                    # Thread-safe GUI update - schedule in main thread
                    if hasattr(self.gui, 'root'):
                        self.gui.root.after(0, lambda: self.gui.root.update_idletasks())
                    self._cleanup_memory_for_long_videos(frame_idx)
                    
                    # Additional cancellation check during GUI updates
                    if self._check_cancellation():
                        self.logger.info("Detection cancelled during GUI update")
                        break
                
                # Record frame processing time for performance monitoring
                if frame_idx > 1:  # Skip first frame as it includes initialization
                    frame_processing_time = time.time() - frame_start_time
                    self.performance_monitor.record_frame_time(frame_processing_time)
                
                # Progress updates every 30 frames (1 second at 30fps)
                if frame_idx % 30 == 0:
                    progress_percent = (frame_idx / self.total_frames) * 100 if self.total_frames > 0 else 0
                    detection_method = "Enhanced" if use_enhanced_detection else "Standard"
                    status_msg = f"{detection_method} Analysis: frame {frame_idx}/{self.total_frames} ({progress_percent:.1f}%)"
                    self._update_status_thread_safe(status_msg)
                
                # ENHANCED DETECTION: Process frame using FastPolygonEngine logic
                motion_detected = False
                bat_center = None
                detected_polygon_idx = -1
                
                if use_enhanced_detection:
                    # Enhanced detection: Full-frame background subtraction then polygon masking
                    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                    fgmask = self.dedicated_back_sub.apply(gray)
                    
                    # Apply morphological operations
                    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (NOISE_KERNEL, NOISE_KERNEL))
                    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)
                    
                    # Apply polygon constraint using binary masks
                    if self.expanded_combined_mask is not None:
                        fgmask = cv2.bitwise_and(fgmask, self.expanded_combined_mask)
                    
                    # Find contours and analyze using enhanced method
                    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
                    
                    for contour in contours:
                        result = self._enhanced_contour_analysis(contour)
                        if result:
                            bat_center = result['center']
                            detected_polygon_idx = result['polygon_idx']
                            motion_detected = True
                            break
                else:
                    # Standard ROI-based detection (fallback when no polygons)
                    gray = cv2.cvtColor(frame[y:y+h, x:x+w], cv2.COLOR_BGR2GRAY)
                    fgmask = self.back_sub.apply(gray)
                    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (NOISE_KERNEL, NOISE_KERNEL))
                    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)

                    contours, _ = cv2.findContours(fgmask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

                    for cnt in contours:
                        area = cv2.contourArea(cnt)
                        if MIN_CONTOUR_AREA < area < MAX_CONTOUR_AREA:
                            M = cv2.moments(cnt)
                            if M["m00"] == 0:
                                continue
                            cx = int(M["m10"] / M["m00"]) + x
                            cy = int(M["m01"] / M["m00"]) + y
                            bat_center = (cx, cy)
                            motion_detected = True
                            break

                # Motion stabilization
                self.motion_history.append(motion_detected)
                if len(self.motion_history) > STABILIZATION_WINDOW:
                    self.motion_history.pop(0)
                stabilized_motion = sum(self.motion_history) > (STABILIZATION_WINDOW // 2)

                if self.cooldown_counter > 0:
                    self.cooldown_counter -= 1

                # Enhanced polygon filtering and event handling
                if stabilized_motion and not self.bat_inside and self.cooldown_counter == 0:
                    entry_valid = True
                    polygon_area = detected_polygon_idx  # Use enhanced detection result
                    
                    # For standard detection, check polygon filtering if enabled
                    if not use_enhanced_detection and self.use_polygon_filtering and self.polygon_areas and bat_center:
                        entry_valid = False
                        for poly_idx, polygon in enumerate(self.polygon_areas):
                            if len(polygon) >= 3 and self.point_in_polygon(bat_center, polygon):
                                polygon_area = poly_idx
                                entry_valid = True
                                break
                    
                    if entry_valid:
                        self.bat_inside = True
                        event_id = len(self.events)
                        entry_time = frame_idx / self.fps
                        
                        # Enhanced tracking integration
                        bat_id = None
                        if use_enhanced_detection and bat_center:
                            bat_id = self._enhanced_bat_tracking(bat_center, frame_idx, polygon_area)
                            # Update polygon detection status for German display
                            if polygon_area >= 0:
                                current_polygon_detections[polygon_area] = frame_idx
                            # Clean up old tracks periodically
                            if frame_idx % 60 == 0:
                                self._cleanup_enhanced_tracking(frame_idx)
                        
                        event_data = {
                            "entry": entry_time,
                            "einflugzeit": entry_time,
                            "exit": None,
                            "ausflugzeit": None,
                            "duration": None,
                            "dauer": None,
                            "frame_idx": frame_idx,
                            "entry_frame": frame_idx,
                            "exit_frame": None,
                            "roi": self.roi,
                            "bat_center": bat_center,
                            "event_id": event_id,
                            "bat_id": bat_id,
                            "detection_method": "enhanced" if use_enhanced_detection else "standard"
                        }
                        
                        # Store detection frame
                        marked = frame.copy()
                        if bat_center:
                            cv2.circle(marked, bat_center, 7, (0, 255, 0), 2)
                        
                        detection_frames = {
                            "trigger_frame": frame.copy(),
                            "marked_frame": marked.copy(),
                            "timestamp": entry_time,
                            "frame_index": frame_idx
                        }
                        self.event_frames[event_id] = detection_frames
                        
                        # Add polygon information and create German status message
                        if polygon_area >= 0:
                            event_data["polygon_area"] = polygon_area
                            if use_enhanced_detection and bat_id:
                                status_msg = f"Fledermaus #{bat_id} im Polygon #{polygon_area + 1} bei {entry_time:.2f}s"
                            else:
                                status_msg = f"Fledermaus im Polygon #{polygon_area + 1} bei {entry_time:.2f}s"
                        else:
                            if use_enhanced_detection and bat_id:
                                status_msg = f"Fledermaus #{bat_id} erkannt bei {entry_time:.2f}s"
                            else:
                                status_msg = f"Fledermaus erkannt bei {entry_time:.2f}s"
                        
                        self.events.append(event_data)
                        self.cooldown_counter = COOLDOWN_FRAMES
                        self.gui.update_status(status_msg)

                elif not stabilized_motion and self.bat_inside and self.cooldown_counter == 0:
                    self.bat_inside = False
                    exit_time = frame_idx / self.fps
                    
                    if self.events:
                        last_event = self.events[-1]
                        last_event["exit"] = exit_time
                        last_event["ausflugzeit"] = exit_time
                        last_event["exit_frame"] = frame_idx
                        
                        entry_time = last_event.get("entry", 0)
                        duration = max(0.0, exit_time - entry_time)
                        last_event["duration"] = duration
                        last_event["dauer"] = duration
                        
                        # Update polygon status for German display
                        polygon_area = last_event.get("polygon_area", -1)
                        if polygon_area >= 0 and polygon_area in current_polygon_detections:
                            del current_polygon_detections[polygon_area]
                        
                        # Store exit frame
                        current_event_id = last_event.get("event_id")
                        if current_event_id is not None and current_event_id in self.event_frames:
                            marked_exit = frame.copy()
                            self.event_frames[current_event_id]["exit_frame"] = frame.copy()
                            self.event_frames[current_event_id]["exit_marked_frame"] = marked_exit
                            self.event_frames[current_event_id]["exit_timestamp"] = exit_time
                        
                        # German exit status message
                        bat_id = last_event.get("bat_id")
                        if polygon_area >= 0:
                            if bat_id:
                                status_msg = f"Fledermaus #{bat_id} aus Polygon #{polygon_area + 1} bei {exit_time:.2f}s"
                            else:
                                status_msg = f"Fledermaus aus Polygon #{polygon_area + 1} bei {exit_time:.2f}s"
                        else:
                            if bat_id:
                                status_msg = f"Fledermaus #{bat_id} verlassen bei {exit_time:.2f}s"
                            else:
                                status_msg = f"Fledermaus verlassen bei {exit_time:.2f}s"
                        
                        self.cooldown_counter = COOLDOWN_FRAMES
                        self._update_status_thread_safe(status_msg)

                
                # Video display with German status overlays
                # Check checkbox state dynamically during processing
                show_video_window = (self.show_overlay and 
                                   getattr(self.gui, 'show_window', tk.BooleanVar(value=True)).get())
                
                if show_video_window:
                    marked = frame.copy()
                    
                    # Draw ROI rectangle (for standard detection or as reference)
                    if not use_enhanced_detection:
                        cv2.rectangle(marked, (x, y), (x + w, y + h), (0, 255, 0), 2)
                    
                    # Draw polygon areas with status-aware coloring
                    if has_polygons:
                        for poly_idx, polygon in enumerate(self.polygon_areas):
                            if len(polygon) >= 3:
                                pts = np.array(polygon, np.int32)
                                
                                # Color based on detection status
                                if poly_idx in current_polygon_detections:
                                    # Active detection in this polygon
                                    color = (0, 0, 255)  # Red for active
                                    thickness = 3
                                    status_text = "Fledermaus im Polygon"
                                else:
                                    # No current detection
                                    color = (0, 255, 0)  # Green for inactive
                                    thickness = 2
                                    status_text = None
                                
                                cv2.polylines(marked, [pts], True, color, thickness)
                                
                                # Add polygon number and status
                                if len(polygon) > 0:
                                    center_x = int(sum(p[0] for p in polygon) / len(polygon))
                                    center_y = int(sum(p[1] for p in polygon) / len(polygon))
                                    cv2.putText(marked, f"#{poly_idx + 1}", (center_x, center_y),
                                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                                    
                                    # Add German status text
                                    if status_text and poly_idx in current_polygon_detections:
                                        cv2.putText(marked, status_text, (center_x, center_y + 25),
                                                  cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                    
                    # Draw bat detection with tracking info
                    if bat_center:
                        cv2.circle(marked, bat_center, 10, (0, 0, 255), -1)
                        
                        # Add German detection text with tracking info
                        detection_text = "Fledermaus erkannt!"
                        if use_enhanced_detection and detected_polygon_idx >= 0:
                            detection_text = f"Fledermaus im Polygon #{detected_polygon_idx + 1}!"
                        
                        cv2.putText(marked, detection_text, 
                                  (bat_center[0] - 80, bat_center[1] - 20),
                                  cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)
                    
                    # Add detection method indicator
                    method_text = "Enhanced Detection" if use_enhanced_detection else "Standard Detection"
                    cv2.putText(marked, method_text, (10, 30),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                    
                    # Show live display with comprehensive error handling
                    try:
                        cv2.imshow("Fledermaus-Erkennung", marked)
                        key = cv2.waitKey(1) & 0xFF
                        
                        # Check if window was closed by X button
                        window_property = cv2.getWindowProperty("Fledermaus-Erkennung", cv2.WND_PROP_VISIBLE)
                        if window_property < 1:  # Window was closed
                            self.processing = False
                            cv2.destroyAllWindows()
                            break
                        
                        if key == ord('q') or key == 27:  # 'q' or ESC key
                            self.processing = False
                            cv2.destroyAllWindows()
                            break
                            
                    except cv2.error:
                        self.processing = False
                        cv2.destroyAllWindows()
                        break
                    except Exception:
                        self.processing = False
                        cv2.destroyAllWindows()
                        break
                
                # Store marked frame for export
                marked_export = frame.copy()
                if bat_center:
                    cv2.circle(marked_export, bat_center, 7, (0, 255, 0), 2)
                    self.bat_centers.append(bat_center)
                
                # Draw ROI for export
                cv2.rectangle(marked_export, (x, y), (x + w, y + h), (0, 255, 255), 1)
                
                # Draw polygons for export
                if has_polygons:
                    for poly_idx, polygon in enumerate(self.polygon_areas):
                        if len(polygon) >= 3:
                            pts = np.array(polygon, np.int32)
                            cv2.fillPoly(marked_export, [pts], (0, 255, 0), cv2.LINE_AA)
                            cv2.polylines(marked_export, [pts], True, (0, 255, 0), 2)
                            
                            if len(polygon) > 0:
                                center_x = int(sum(p[0] for p in polygon) / len(polygon))
                                center_y = int(sum(p[1] for p in polygon) / len(polygon))
                                cv2.putText(marked_export, f"#{poly_idx + 1}", (center_x, center_y),
                                          cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                
                self.marked_frames.append(marked_export)

        except Exception as e:
            self._update_status_thread_safe(f"Fehler bei der Erkennung: {str(e)}")
        finally:
            # Handle incomplete events
            self._finalize_incomplete_events(frame_idx)
            self.processing = False
            
            # Close video resources
            cv2.destroyAllWindows()
            if self.cap:
                self.cap.release()
                
            # Final status with German text and detection summary
            total_time = time.time() - start_time
            detection_text = "Enhanced" if use_enhanced_detection else "Standard"
            tracked_bats = len(self.active_bats) if hasattr(self, 'active_bats') else 0
            
            if tracked_bats > 0:
                status_msg = f"Analyse abgeschlossen ({detection_text}): {len(self.events)} Ereignisse, {tracked_bats} verfolgte FledermÃ¤use in {total_time:.1f}s"
            else:
                status_msg = f"Analyse abgeschlossen ({detection_text}): {len(self.events)} Ereignisse in {total_time:.1f}s"
            
            self._update_status_thread_safe(status_msg)
            
            # Notify GUI completion in main thread
            if hasattr(self.gui, 'on_detection_finished'):
                self._schedule_gui_update(self.gui.on_detection_finished)

    def is_inside_zone(self, x, y, zone_coords):
        x1, y1, x2, y2 = zone_coords
        return x1 <= x <= x2 and y1 <= y <= y2



   
    def get_event_frames(self, event_id):
        """Get all captured frames for a specific event"""
        if event_id in self.event_frames:
            return self.event_frames[event_id]
        return None
    
    def extract_event_frame_sequence(self, event_idx, num_frames=5):
        """Extract a sequence of frames around an event for validation"""
        if event_idx >= len(self.events):
            return None
            
        event = self.events[event_idx]
        event_id = event.get("event_id")
        
        # First try to get stored frames
        if event_id is not None and event_id in self.event_frames:
            stored_frames = self.event_frames[event_id]
            
            # Return stored frame sequence if available
            if "frame_sequence" in stored_frames and stored_frames["frame_sequence"]:
                return {
                    "event_id": event_id,
                    "frames": stored_frames["frame_sequence"][:num_frames],
                    "trigger_frame": stored_frames.get("trigger_frame"),
                    "marked_frame": stored_frames.get("marked_frame"),
                    "exit_frame": stored_frames.get("exit_frame"),
                    "source": "stored"
                }
        
        # Fallback: extract frames from video if stored frames not available
        try:
            cap = cv2.VideoCapture(self.video_path)
            if not cap.isOpened():
                return None
                
            fps = cap.get(cv2.CAP_PROP_FPS) or 30
            start_time = event.get('entry', 0)
            end_time = event.get('exit', start_time + 1)
            
            # Calculate frame indices
            start_frame = max(0, int(start_time * fps) - 2)  # Start 2 frames before
            end_frame = min(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)), int(end_time * fps) + 2)
            
            frames = []
            for frame_idx in range(start_frame, min(start_frame + num_frames, end_frame + 1)):
                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()
                if ret:
                    # Apply highlighting
                    marked_frame = self.highlight_event_frame(frame, event)
                    frames.append({
                        "frame": frame.copy(),
                        "marked_frame": marked_frame,
                        "timestamp": frame_idx / fps,
                        "frame_index": frame_idx,
                        "bat_center": event.get("bat_center")
                    })
            
            cap.release()
            
            return {
                "event_id": event_idx,  # Use index if no event_id
                "frames": frames,
                "trigger_frame": frames[2] if len(frames) > 2 else frames[0] if frames else None,
                "source": "extracted"
            }
            
        except Exception as e:
            return None
    
    def highlight_event_frame(self, frame, event):
        """Apply ROI and detection highlighting to a frame"""
        highlighted = frame.copy()
        
        try:
            # Draw ROI if available
            if self.roi:
                x, y, w, h = self.roi
                cv2.rectangle(highlighted, (x, y), (x + w, y + h), (0, 255, 255), 2)
            
            # Draw polygons if available
            if self.polygon_areas:
                for poly_idx, polygon in enumerate(self.polygon_areas):
                    if len(polygon) >= 3:
                        pts = np.array(polygon, np.int32)
                        cv2.polylines(highlighted, [pts], True, (0, 255, 0), 2)
                        
                        # Add polygon number
                        if len(polygon) > 0:
                            center_x = int(sum(p[0] for p in polygon) / len(polygon))
                            center_y = int(sum(p[1] for p in polygon) / len(polygon))
                            cv2.putText(highlighted, f"#{poly_idx + 1}", (center_x, center_y),
                                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            
            # Highlight bat center if available
            bat_center = event.get('bat_center')
            if bat_center:
                cv2.circle(highlighted, bat_center, 15, (255, 0, 0), 3)
                cv2.circle(highlighted, bat_center, 5, (255, 255, 255), -1)
                cv2.putText(highlighted, "BAT", (bat_center[0] + 20, bat_center[1]),
                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
            
        except Exception as e:
            pass  # Ignore highlighting errors
        
        return highlighted


    def export_results(self):
        if not self.events:
            safe_messagebox(messagebox.showinfo, "Export Results", "No events to export.", "No events to export")
            return

        # Extract base video name without extension
        video_name = os.path.splitext(os.path.basename(self.video_path))[0]
        
        # Get today's date and time
        current_datetime = datetime.now().strftime('%Y%m%d_%H%M%S')

        # Construct filename with detection mode and area info
        detection_mode = getattr(self, 'active_detection_mode', 'unknown')
        if detection_mode == "roi_priority" or detection_mode == "roi_only":
            mode_suffix = "_ROI"
        elif detection_mode == "polygon_only":
            mode_suffix = f"_polygons{len(self.polygon_areas)}"
        elif detection_mode == "full_frame":
            mode_suffix = "_fullframe"
        else:
            mode_suffix = f"_with_{len(self.polygon_areas)}polygons" if self.polygon_areas else ""
            
        csv_filename = f"{video_name}_detection_{current_datetime}{mode_suffix}.csv"
        csv_path = os.path.join(os.path.dirname(self.video_path), csv_filename)

        # Export the data with detection mode and polygon information
        export_events_to_csv(self.events, self.video_path, csv_path, self.polygon_areas)

        # Show enhanced summary with detection mode info
        area_info = self.get_active_detection_area_info()
        mode = area_info.get("mode", "unknown")
        
        if mode == "roi_priority":
            detection_info = f"Detection Mode: ROI Priority (Polygons ignored: {area_info.get('polygon_count', 0)})"
        elif mode == "roi_only":
            detection_info = "Detection Mode: ROI Only"
        elif mode == "polygon_only":
            count = area_info.get('polygon_count', 0)
            detection_info = f"Detection Mode: {count} Polygon Area{'s' if count > 1 else ''}"
        elif mode == "full_frame":
            detection_info = "Detection Mode: Full Frame"
        else:
            detection_info = "Detection Mode: Unknown"
        
        events_in_polygons = len([e for e in self.events if e.get("polygon_area", -1) >= 0])
        
        summary_msg = (f"Events exported successfully to:\n{csv_path}\n\n"
                      f"Total events: {len(self.events)}\n"
                      f"{detection_info}")
        
        if events_in_polygons > 0:
            summary_msg += f"\nEvents in polygon areas: {events_in_polygons}"
        
        # ðŸ”§ CRITICAL FIX: Use safe messagebox wrapper to prevent crashes
        safe_messagebox(messagebox.showinfo, "Export Results", summary_msg, "âœ… Export completed successfully")

    def export_marked_video(self):
        """Export marked video with polygon areas highlighted to per-video results folder"""
        if not self.marked_frames:
            messagebox.showinfo("Export Marked Video", "No marked frames available for export.")
            return
            
        try:
            self.gui.update_status("Exporting marked video...")
            
            # Get username for filename
            try:
                import getpass
                username = getpass.getuser()
            except:
                username = "user"
            
            # Use enhanced export_video function with per-video folder structure
            # Get user choice from GUI if available
            user_choice = getattr(self.gui, 'user_folder_choice', None)
            output_path = export_video(self.marked_frames, self.fps, self.video_path, username, user_choice=user_choice)
            
            if output_path:
                polygon_info = f" with {len(self.polygon_areas)} polygon areas" if self.polygon_areas else ""
                messagebox.showinfo("Export Marked Video", 
                                  f"Marked video{polygon_info} exported successfully to:\n{output_path}")
                self.gui.update_status("Marked video export finished.")
            else:
                messagebox.showerror("Export Marked Video", "Failed to export marked video.")
                self.gui.update_status("Marked video export failed.")
                
        except Exception as e:
            error_msg = f"Failed to export marked video: {str(e)}"
            messagebox.showerror("Export Marked Video", error_msg)
            self.gui.update_status("Marked video export failed.")



    def export_flightMap(self):
        if not self.bat_centers or not self.events:
            messagebox.showinfo("Export Flight Map", "No bat trajectory data available.")
            return
        fps = 15
        bat_paths_with_time = {
            1: [(i / fps, pos[0], pos[1]) for i, pos in enumerate(self.bat_centers)]
        }
        output_dir = os.path.join(os.path.dirname(self.video_path), "exports")
        os.makedirs(output_dir, exist_ok=True)
        filename_base = os.path.splitext(os.path.basename(self.video_path))[0]
        img_path = export_flightMap(bat_paths_with_time, output_dir, filename_base=filename_base, user="IfAÃ–")
        if img_path:
            messagebox.showinfo("Export Flight Map", f"Flight map saved:\n{img_path}")
        else:
            messagebox.showinfo("Export Flight Map", "Failed to generate flight map.")


    def export_hotzone(self):
        """Export hotzone functionality - placeholder for future implementation"""
        pass

    def filter_events_by_roi(self):
        """Filter events by rectangular ROI (legacy method)"""
        if not self.roi:
            return
        x, y, w, h = self.roi
        x2, y2 = x + w, y + h
        filtered_events = []
        for idx, event in enumerate(self.events):
            center = event.get("bat_center")
            if center:
                cx, cy = center
                if x <= cx <= x2 and y <= cy <= y2:
                    filtered_events.append(event)
        self.events = filtered_events

    def set_polygon_areas(self, polygon_areas):
        """Set polygon areas for event filtering and create binary mask"""
        self.polygon_areas = polygon_areas.copy() if polygon_areas else []
        # Reset polygon filtering flag - will be set appropriately in start_detection
        self.use_polygon_filtering = False
        
        # Update polygon mask when areas change
        self.update_polygon_mask()
        

    def test_polygon_mask_generation(self):
        """
        Test function to verify polygon mask generation.
        Call this after loading video and setting polygon areas.
        """
        if self.frame_shape is None:
            return False
            
        if not self.polygon_areas:
            return False
        
        # Generate test mask
        test_mask = self.create_polygon_mask(self.frame_shape, self.polygon_areas)
        
        if test_mask is not None:
            # Count non-zero pixels (inside polygon areas)
            mask_area = np.count_nonzero(test_mask)
            total_area = self.frame_shape[0] * self.frame_shape[1]
            coverage_percent = (mask_area / total_area) * 100
            
            
            return True
        else:
            return False

    def enable_polygon_filtering(self, enabled=True):
        """Enable or disable polygon filtering mode"""
        self.polygon_filtering_enabled = enabled

    def _finalize_incomplete_events(self, final_frame_idx):
        """
        Finalize any events that don't have exit times by using the final frame
        This ensures all events have proper entry/exit times for display and export
        """
        for event in self.events:
            if event.get("exit") is None:
                # Use final frame as exit time for incomplete events
                final_time = final_frame_idx / self.fps if self.fps > 0 else 0
                event["exit"] = final_time
                event["ausflugzeit"] = final_time
                event["exit_frame"] = final_frame_idx
                
                # Calculate duration
                entry_time = event.get("entry", 0)
                duration = max(0.0, final_time - entry_time)
                event["duration"] = duration
                event["dauer"] = duration
                
                # Mark as incomplete for reference
                event["incomplete"] = True
                event["remarks"] = event.get("remarks", "") + " [Event unvollstÃ¤ndig - Video Ende erreicht]"
                

    def point_in_polygon_optimized(self, point, polygon_idx=None):
        """
        OPTIMIZED: Check if a point is inside polygons using faster cv2.pointPolygonTest.
        
        Args:
            point: (x, y) tuple
            polygon_idx: If specified, check only this polygon index. Otherwise check all.
            
        Returns:
            tuple: (is_inside, polygon_index) or (False, -1) if outside all polygons
        """
        x, y = point
        
        # OPTIMIZATION: First check if point is within bounding rectangle
        if (self.use_bounding_rect_optimization and 
            self.polygon_bounding_rect is not None):
            bx, by, bw, bh = self.polygon_bounding_rect
            if not (bx <= x <= bx + bw and by <= y <= by + bh):
                return False, -1
        
        # Check specific polygon if index provided
        if polygon_idx is not None and 0 <= polygon_idx < len(self.polygon_areas_cv2):
            result = cv2.pointPolygonTest(self.polygon_areas_cv2[polygon_idx], (x, y), False)
            return result >= 0, polygon_idx if result >= 0 else -1
        
        # Check all polygons using pre-converted CV2 arrays
        for poly_idx, polygon_cv2 in enumerate(self.polygon_areas_cv2):
            result = cv2.pointPolygonTest(polygon_cv2, (x, y), False)
            if result >= 0:
                return True, poly_idx
                
        return False, -1

    def point_in_polygon(self, point, polygon):
        """Check if a point is inside a polygon using ray casting algorithm"""
        x, y = point
        n = len(polygon)
        inside = False
        
        p1x, p1y = polygon[0]
        for i in range(1, n + 1):
            p2x, p2y = polygon[i % n]
            if y > min(p1y, p2y):
                if y <= max(p1y, p2y):
                    if x <= max(p1x, p2x):
                        if p1y != p2y:
                            xinters = (y - p1y) * (p2x - p1x) / (p2y - p1y) + p1x
                        if p1x == p2x or x <= xinters:
                            inside = not inside
            p1x, p1y = p2x, p2y
            
        return inside

    def create_polygon_mask(self, frame_shape, polygon_areas):
        """
        Create a binary mask from polygon areas for frame-level masking.
        
        Args:
            frame_shape: Tuple (height, width) of video frame
            polygon_areas: List of polygons, each as list of (x,y) points
            
        Returns:
            numpy.ndarray: Binary mask where 255=inside polygons, 0=outside
        """
        if not polygon_areas:
            return None
            
        # Create empty mask
        mask = np.zeros((frame_shape[0], frame_shape[1]), dtype=np.uint8)
        
        # Fill each polygon area
        for polygon in polygon_areas:
            if len(polygon) >= 3:
                # Convert polygon points to numpy array format for OpenCV
                pts = np.array(polygon, dtype=np.int32)
                # Fill polygon area with 255 (white)
                cv2.fillPoly(mask, [pts], 255)
        
        return mask

    def apply_polygon_mask_to_frame(self, frame, mask):
        """
        Apply polygon mask to a frame, setting outside areas to black.
        
        Args:
            frame: Input frame (grayscale or BGR)
            mask: Binary mask (255=keep, 0=remove)
            
        Returns:
            numpy.ndarray: Masked frame
        """
        if mask is None:
            return frame
            
        # Apply mask - areas outside polygons become 0 (black)
        if len(frame.shape) == 3:  # BGR frame
            masked_frame = cv2.bitwise_and(frame, frame, mask=mask)
        else:  # Grayscale frame
            masked_frame = cv2.bitwise_and(frame, frame, mask=mask)
            
        return masked_frame

    def update_polygon_mask(self):
        """
        Update the polygon mask when polygon areas change or frame shape is known.
        Call this whenever polygon_areas is modified or video is loaded.
        
        OPTIMIZATION: Also pre-computes bounding rectangle and OpenCV-optimized polygons.
        """
        if self.frame_shape is not None and self.polygon_areas:
            self.polygon_mask = self.create_polygon_mask(self.frame_shape, self.polygon_areas)
            
            # OPTIMIZATION: Pre-compute bounding rectangle for all polygons
            self._compute_polygon_bounding_rect()
            
            # OPTIMIZATION: Pre-convert polygons to numpy arrays for faster cv2.pointPolygonTest
            self.polygon_areas_cv2 = []
            for polygon in self.polygon_areas:
                if len(polygon) >= 3:
                    poly_array = np.array(polygon, dtype=np.int32)
                    self.polygon_areas_cv2.append(poly_array)
            
            if self.polygon_bounding_rect:
                x, y, w, h = self.polygon_bounding_rect
        else:
            self.polygon_mask = None
            self.polygon_bounding_rect = None
            self.polygon_areas_cv2 = []

    def _compute_polygon_bounding_rect(self):
        """
        OPTIMIZATION: Compute bounding rectangle encompassing all polygons.
        This allows processing only the relevant area instead of the full frame.
        """
        if not self.polygon_areas:
            self.polygon_bounding_rect = None
            return
            
        min_x = float('inf')
        min_y = float('inf')
        max_x = float('-inf')
        max_y = float('-inf')
        
        for polygon in self.polygon_areas:
            for point in polygon:
                x, y = point
                min_x = min(min_x, x)
                min_y = min(min_y, y)
                max_x = max(max_x, x)
                max_y = max(max_y, y)
        
        # Add small padding to ensure we don't miss edge cases
        padding = 10
        min_x = max(0, min_x - padding)
        min_y = max(0, min_y - padding)
        max_x = min(self.frame_shape[1], max_x + padding) if self.frame_shape else max_x + padding
        max_y = min(self.frame_shape[0], max_y + padding) if self.frame_shape else max_y + padding
        
        self.polygon_bounding_rect = (int(min_x), int(min_y), int(max_x - min_x), int(max_y - min_y))

    def filter_events_by_polygons(self):
        """Filter events by polygon areas"""
        if not self.polygon_areas:
            return
            
        filtered_events = []
        for idx, event in enumerate(self.events):
            center = event.get("bat_center")
            if center:
                cx, cy = center
                is_inside_any_polygon = False
                
                for polygon_idx, polygon in enumerate(self.polygon_areas):
                    if len(polygon) >= 3 and self.point_in_polygon((cx, cy), polygon):
                        is_inside_any_polygon = True
                        # Add polygon information to event
                        event["polygon_area"] = polygon_idx
                        break
                
                if is_inside_any_polygon:
                    filtered_events.append(event)
        
        self.events = filtered_events



    
    

  

    def display_csv_results(self, csv_file):
        """Inhalt einer CSV-Ergebnisdatei anzeigen"""
        try:
           # Read the CSV file using standard Python functions
            import csv
            
          # Create a new window to display results
            result_window = tk.Toplevel(self.gui.root)
            result_window.title(f"Ergebnisse: {os.path.basename(csv_file)}")
            result_window.geometry("800x600")
            
            # Create a frame for the results
            frame = tk.Frame(result_window)
            frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
            
            # Create a text widget to display CSV content
            text = tk.Text(frame, wrap=tk.NONE)
            text.pack(fill=tk.BOTH, expand=True)
            
            # Add scrollbars
            yscroll = tk.Scrollbar(text, command=text.yview)
            yscroll.pack(side=tk.RIGHT, fill=tk.Y)
            xscroll = tk.Scrollbar(frame, command=text.xview, orient=tk.HORIZONTAL)
            xscroll.pack(side=tk.BOTTOM, fill=tk.X)
            text.config(yscrollcommand=yscroll.set, xscrollcommand=xscroll.set)
            
            # Read and display CSV data
            with open(csv_file, 'r') as f:
                csv_reader = csv.reader(f)
                headers = next(csv_reader) # Get the header row
                
                # Format and insert the headers
                header = "\t".join(headers)
                text.insert(tk.END, header + "\n")
                text.insert(tk.END, "-" * len(header) + "\n")
                
                # Insert data rows
                for row in csv_reader:
                    line = "\t".join(row)
                    text.insert(tk.END, line + "\n")
                
            # Add button to load associated video
            video_base = os.path.basename(csv_file).split("_")[0]
            video_dir = os.path.dirname(csv_file)
            
            # Look for possible video files
            video_files = []
            for ext in ['.avi', '.mp4', '.mov']:
                possible_file = os.path.join(video_dir, video_base + ext)
                if os.path.exists(possible_file):
                    video_files.append(possible_file)
                
 
        except Exception as e:
            messagebox.showerror("Fehler", f"Fehler beim Anzeigen der Ergebnisse: {str(e)}")

    # ========== ENHANCED POLYGON VISUALIZATION METHODS ==========
    
    def draw_polygon_visualization(self, frame, bat_center=None, detected_polygon_idx=-1, overlay_alpha=None):
        """
        Enhanced polygon visualization method specifically for polygon-based detection.
        Draws polygon outlines, optional semi-transparent overlays, and highlights bats only inside polygons.
        
        Args:
            frame: Input frame to draw on
            bat_center: (x, y) center of detected bat, or None
            detected_polygon_idx: Index of polygon where bat was detected, -1 if none
            overlay_alpha: Transparency level for polygon overlays (0.0-1.0), uses config default if None
            
        Returns:
            Annotated frame with polygon visualization
        """
        if not self.polygon_areas:
            return frame
            
        annotated_frame = frame.copy()
        
        # Use configured overlay alpha if not specified
        if overlay_alpha is None:
            overlay_alpha = POLYGON_OVERLAY_ALPHA
        
        # Create overlay layer for semi-transparent fills
        if POLYGON_ENABLE_OVERLAY:
            overlay = annotated_frame.copy()
        
        for poly_idx, polygon in enumerate(self.polygon_areas):
            if len(polygon) < 3:
                continue
                
            pts = np.array(polygon, dtype=np.int32)
            
            # Determine colors based on detection state
            if poly_idx == detected_polygon_idx:
                # Active polygon (with bat detection)
                outline_color = POLYGON_ACTIVE_COLOR
                fill_color = POLYGON_ACTIVE_COLOR
                line_thickness = POLYGON_OUTLINE_THICKNESS_ACTIVE
            else:
                # Inactive polygon
                outline_color = POLYGON_INACTIVE_COLOR
                fill_color = POLYGON_INACTIVE_COLOR
                line_thickness = POLYGON_OUTLINE_THICKNESS_INACTIVE
            
            # Draw semi-transparent fill on overlay
            if POLYGON_ENABLE_OVERLAY:
                cv2.fillPoly(overlay, [pts], fill_color)
            
            # Draw polygon outline
            cv2.polylines(annotated_frame, [pts], True, outline_color, line_thickness)
            
            # Add polygon number label
            center_x = int(sum(p[0] for p in polygon) / len(polygon))
            center_y = int(sum(p[1] for p in polygon) / len(polygon))
            
            # Background for text
            text = f"#{poly_idx + 1}"
            font = POLYGON_LABEL_FONT
            font_scale = POLYGON_LABEL_SCALE
            font_thickness = POLYGON_LABEL_THICKNESS
            (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, font_thickness)
            
            # Text background rectangle
            cv2.rectangle(annotated_frame, 
                         (center_x - text_width//2 - 5, center_y - text_height//2 - 5),
                         (center_x + text_width//2 + 5, center_y + text_height//2 + 5),
                         (0, 0, 0), -1)
            
            # Text
            cv2.putText(annotated_frame, text, 
                       (center_x - text_width//2, center_y + text_height//2),
                       font, font_scale, (255, 255, 255), font_thickness)
        
        # Blend overlay with original frame
        if POLYGON_ENABLE_OVERLAY:
            cv2.addWeighted(overlay, overlay_alpha, annotated_frame, 1 - overlay_alpha, 0, annotated_frame)
        
        # Highlight bat only if detected inside a polygon
        if bat_center and detected_polygon_idx >= 0:
            self.draw_enhanced_bat_highlight(annotated_frame, bat_center, detected_polygon_idx)
        
        return annotated_frame
    
    def draw_enhanced_bat_highlight(self, frame, bat_center, polygon_idx):
        """
        Enhanced bat highlighting specifically for polygon-based detection.
        
        Args:
            frame: Frame to draw on (modified in place)
            bat_center: (x, y) center of detected bat
            polygon_idx: Index of polygon where bat was detected
        """
        cx, cy = bat_center
        
        # Main bat detection circle
        cv2.circle(frame, (cx, cy), BAT_DETECTION_RADIUS, BAT_DETECTION_COLOR, 3)
        cv2.circle(frame, (cx, cy), BAT_CENTER_RADIUS, BAT_CENTER_COLOR, -1)
        
        # Additional visual indicators
        # Outer detection ring
        cv2.circle(frame, (cx, cy), BAT_OUTER_RING_RADIUS, BAT_DETECTION_COLOR, 1)
        
        # Small cross-hair for precise center
        cross_size = 3
        cv2.line(frame, (cx - cross_size, cy), (cx + cross_size, cy), BAT_CROSSHAIR_COLOR, 2)
        cv2.line(frame, (cx, cy - cross_size), (cx, cy + cross_size), BAT_CROSSHAIR_COLOR, 2)
        
        # Text annotation showing polygon association
        text = f"Poly #{polygon_idx + 1}"
        font = BAT_LABEL_FONT
        font_scale = BAT_LABEL_SCALE
        font_thickness = BAT_LABEL_THICKNESS
        
        # Position text above the bat
        text_y = cy - 25
        (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)
        text_x = cx - text_width // 2
        
        # Text background
        cv2.rectangle(frame, 
                     (text_x - 3, text_y - text_height - 3),
                     (text_x + text_width + 3, text_y + 3),
                     (0, 0, 0), -1)
        
        # Text
        cv2.putText(frame, text, (text_x, text_y), 
                   font, font_scale, BAT_DETECTION_COLOR, font_thickness)
    
    def create_polygon_info_overlay(self, frame, total_detections=0):
        """
        Create an informational overlay showing polygon detection statistics.
        
        Args:
            frame: Input frame
            total_detections: Total number of detections across all polygons
            
        Returns:
            Frame with info overlay
        """
        if not POLYGON_ENABLE_INFO_PANEL or not self.polygon_areas:
            return frame
            
        overlay_frame = frame.copy()
        
        # Info panel background
        panel_height = 30 + (len(self.polygon_areas) * 25)
        panel_width = INFO_PANEL_WIDTH
        panel_x, panel_y = INFO_PANEL_POSITION
        
        # Semi-transparent background
        overlay = overlay_frame.copy()
        cv2.rectangle(overlay, (panel_x, panel_y), 
                     (panel_x + panel_width, panel_y + panel_height), 
                     INFO_PANEL_BACKGROUND_COLOR, -1)
        cv2.addWeighted(overlay, INFO_PANEL_ALPHA, overlay_frame, 1 - INFO_PANEL_ALPHA, 0, overlay_frame)
        
        # Header
        font = cv2.FONT_HERSHEY_SIMPLEX
        cv2.putText(overlay_frame, "Polygon Detection Areas", 
                   (panel_x + 5, panel_y + 20), 
                   font, 0.6, INFO_PANEL_TEXT_COLOR, 2)
        
        # Polygon info
        y_offset = panel_y + 45
        for poly_idx, polygon in enumerate(self.polygon_areas):
            color = POLYGON_INACTIVE_COLOR  # Default color
            
            # Count detections in this polygon (if tracking)
            poly_detections = sum(1 for event in self.events 
                                if event.get("polygon_area") == poly_idx)
            
            text = f"#{poly_idx + 1}: {poly_detections} detections"
            cv2.putText(overlay_frame, text, (panel_x + 5, y_offset), 
                       font, 0.5, color, 1)
            y_offset += 25
        
        return overlay_frame
    
    def create_enhanced_event_frame(self, frame, event_data, frame_type="detection"):
        """
        Create an enhanced frame visualization for specific events.
        This method provides detailed visualization for event export and analysis.
        
        Args:
            frame: Input frame
            event_data: Event dictionary containing detection information
            frame_type: Type of frame ("detection", "entry", "exit", "sequence")
            
        Returns:
            Enhanced frame with detailed polygon and event visualization
        """
        if not self.polygon_areas:
            return frame
            
        # Get event-specific information
        polygon_idx = event_data.get("polygon_area", -1)
        bat_center = event_data.get("bat_center")
        event_id = event_data.get("event_id", -1)
        
        # Create base visualization
        enhanced_frame = self.draw_polygon_visualization(frame, bat_center, polygon_idx)
        
        # Add event-specific annotations
        if frame_type in ["detection", "entry"]:
            # Add entry indicator
            if bat_center:
                cx, cy = bat_center
                # Draw entry arrow
                arrow_start = (cx - 30, cy - 30)
                arrow_end = (cx - 10, cy - 10)
                cv2.arrowedLine(enhanced_frame, arrow_start, arrow_end, (0, 255, 0), 3, tipLength=0.3)
                
                # Add entry text
                cv2.putText(enhanced_frame, "ENTRY", (cx - 40, cy - 35), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
                           
        elif frame_type == "exit":
            # Add exit indicator for polygon area
            if polygon_idx >= 0 and polygon_idx < len(self.polygon_areas):
                polygon = self.polygon_areas[polygon_idx]
                center_x = int(sum(p[0] for p in polygon) / len(polygon))
                center_y = int(sum(p[1] for p in polygon) / len(polygon))
                
                # Draw exit indicator
                cv2.putText(enhanced_frame, "EXIT", (center_x - 25, center_y - 20), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
        
        # Add event information overlay
        info_text = [
            f"Event #{event_id + 1}",
            f"Polygon #{polygon_idx + 1}" if polygon_idx >= 0 else "No Polygon",
            f"Type: {frame_type.title()}"
        ]
        
        # Add event info panel
        panel_x, panel_y = 10, enhanced_frame.shape[0] - 100
        panel_width = 200
        panel_height = len(info_text) * 25 + 10
        
        # Semi-transparent background
        overlay = enhanced_frame.copy()
        cv2.rectangle(overlay, (panel_x, panel_y), 
                     (panel_x + panel_width, panel_y + panel_height),
                     (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, enhanced_frame, 0.3, 0, enhanced_frame)
        
        # Add text
        for i, text in enumerate(info_text):
            y_pos = panel_y + 20 + (i * 25)
            cv2.putText(enhanced_frame, text, (panel_x + 5, y_pos),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return enhanced_frame
    
    def create_export_optimized_frame(self, frame, bat_center=None, detected_polygon_idx=-1, original_frame=None, frame_number=0):
        """
        FULLY OPTIMIZED export frame creation with perfect polygon and bat visualization.
        
        This method ensures:
        1. Clean polygon outlines in bright green with perfect coordinate alignment
        2. NO distracting numbering or labels - professional clean appearance  
        3. Subtle bat highlighting with unique tracking IDs across frames
        4. Overlay blending preserves all highlighting
        5. Export frames clearly show polygon-constrained detection
        6. ENHANCED: Areas outside polygons are dimmed (not black) for better context
        7. COORDINATE FIX: Polygons align exactly with drawn positions (fixed scaling issue)
        8. BAT TRACKING: Consistent bat IDs maintained across frames for user verification
        
        Args:
            frame: Input frame (usually masked frame for polygon mode)
            bat_center: (x, y) center of detected bat, or None
            detected_polygon_idx: Index of polygon where bat was detected, -1 if none
            original_frame: Original unmasked frame for creating dimmed background (polygon mode only)
            frame_number: Current frame number for bat tracking across frames
            
        Returns:
            Frame optimized for video export with perfect polygon alignment and clean bat tracking
        """
        if not self.polygon_areas:
            return frame
        
        # === ENHANCED POLYGON VISUALIZATION ===
        # For polygon mode, create dimmed background with fully visible polygon areas
        if original_frame is not None and self.polygon_mask is not None:
            # Start with the original frame as base
            export_frame = original_frame.copy()
            
            # Create dimmed overlay for areas outside polygons
            dimmed_overlay = export_frame.copy()
            
            # Apply heavy dimming to the entire frame
            dimming_factor = 0.3  # Dim to 30% brightness
            dimmed_overlay = cv2.convertScaleAbs(dimmed_overlay, alpha=dimming_factor, beta=0)
            
            # Create inverted mask (areas outside polygons)
            outside_mask = cv2.bitwise_not(self.polygon_mask)
            
            # Apply dimmed overlay only to areas outside polygons
            export_frame[outside_mask > 0] = dimmed_overlay[outside_mask > 0]
            
            # Ensure polygon areas remain fully visible (use original brightness in polygon areas)
            # No need to modify polygon areas since they retain original brightness
            
        else:
            # Standard processing for non-polygon mode or when original_frame is not provided
            export_frame = frame.copy()
        
        # === STEP 1: DRAW CLEAN POLYGON OUTLINES ===
        # Draw polygon outlines without any distracting labels or numbering
        for poly_idx, polygon in enumerate(self.polygon_areas):
            if len(polygon) < 3:
                continue
                
            pts = np.array(polygon, dtype=np.int32)
            
            # Use consistent bright green for excellent visibility and clarity
            if poly_idx == detected_polygon_idx:
                # Active polygon (with bat detection) - brighter green with thicker line
                outline_color = (0, 255, 0)      # Bright green (BGR format)
                line_thickness = 6               # Extra thick for active polygon
            else:
                # Inactive polygon - same bright green but slightly thinner
                outline_color = (0, 255, 0)      # Bright green (BGR format)  
                line_thickness = 4               # Thick outline for visibility
            
            # Draw clean polygon outline with enhanced thickness for clarity
            # This ensures perfect alignment with drawn coordinates
            cv2.polylines(export_frame, [pts], True, outline_color, line_thickness)
            
            # NO LABELING OR NUMBERING - Keep export video clean and professional
        
        # === STEP 2: APPLY BAT HIGHLIGHTING WITH TRACKING (BEFORE OVERLAY) ===
        # This ensures bat highlighting is never overwritten by overlay blending
        if bat_center and detected_polygon_idx >= 0:
            # Get or assign unique bat ID for tracking across frames
            bat_id = self.track_bat_across_frames(bat_center, detected_polygon_idx, frame_number)
            if bat_id is not None:
                self.draw_export_optimized_bat_highlight(export_frame, bat_center, bat_id, detected_polygon_idx)
        
        # === STEP 3: APPLY SUBTLE SEMI-TRANSPARENT OVERLAY (AFTER BAT HIGHLIGHTING) ===
        # This preserves the bat highlighting while adding very subtle polygon fills
        if POLYGON_ENABLE_OVERLAY and hasattr(self, 'polygon_areas') and self.polygon_areas:
            # Create overlay AFTER bat highlighting is applied
            overlay = export_frame.copy()
            
            # Add very subtle semi-transparent fills with consistent green theme
            for poly_idx, polygon in enumerate(self.polygon_areas):
                if len(polygon) < 3:
                    continue
                    
                pts = np.array(polygon, dtype=np.int32)
                
                # Use consistent green theme with very subtle fills
                if poly_idx == detected_polygon_idx:
                    fill_color = (0, 140, 0)    # Slightly brighter green for active polygon
                else:
                    fill_color = (0, 120, 0)    # Darker green for inactive polygons
                
                # Draw semi-transparent fill on overlay
                cv2.fillPoly(overlay, [pts], fill_color)
            
            # Blend with very low alpha to preserve bat highlighting and maintain clean look
            export_overlay_alpha = 0.06  # Even lower transparency for cleaner appearance
            cv2.addWeighted(overlay, export_overlay_alpha, export_frame, 1 - export_overlay_alpha, 0, export_frame)
        
        # === STEP 4: ADD EXPORT WATERMARK (OPTIONAL) ===
        # This can help identify export frames but is kept minimal
        if hasattr(self, 'add_export_watermark'):
            # Add minimal watermark that doesn't interfere with visualization
            # export_frame = self.add_export_watermark(export_frame)
            pass  # Disabled to keep export frames clean
        
        return export_frame
    
    def track_bat_across_frames(self, bat_center, polygon_idx, frame_number):
        """
        Enhanced bat tracking system to maintain consistent IDs across frames.
        
        Args:
            bat_center: (x, y) position of detected bat
            polygon_idx: Index of polygon where bat was detected
            frame_number: Current frame number
            
        Returns:
            bat_id: Unique identifier for this bat (int)
        """
        if not bat_center:
            return None
            
        cx, cy = bat_center
        
        # Clean up expired bat IDs
        expired_ids = []
        for bat_id, bat_info in self.bat_tracker.items():
            if frame_number - bat_info['last_seen'] > self.bat_tracking_timeout:
                expired_ids.append(bat_id)
        
        for bat_id in expired_ids:
            del self.bat_tracker[bat_id]
        
        # Try to match with existing bat
        best_match_id = None
        best_distance = float('inf')
        
        for bat_id, bat_info in self.bat_tracker.items():
            # Only consider bats in the same polygon
            if bat_info['polygon_idx'] == polygon_idx:
                old_x, old_y = bat_info['position']
                distance = ((cx - old_x) ** 2 + (cy - old_y) ** 2) ** 0.5
                
                if distance < self.bat_tracking_threshold and distance < best_distance:
                    best_distance = distance
                    best_match_id = bat_id
        
        if best_match_id is not None:
            # Update existing bat
            self.bat_tracker[best_match_id] = {
                'position': (cx, cy),
                'last_seen': frame_number,
                'polygon_idx': polygon_idx
            }
            return best_match_id
        else:
            # Create new bat ID
            new_bat_id = self.next_bat_id
            self.next_bat_id += 1
            self.bat_tracker[new_bat_id] = {
                'position': (cx, cy),
                'last_seen': frame_number,
                'polygon_idx': polygon_idx
            }
            return new_bat_id
    
    def draw_export_optimized_bat_highlight(self, frame, bat_center, bat_id, polygon_idx):
        """
        Enhanced subtle bat highlighting optimized for video export with clean markers.
        
        Args:
            frame: Frame to draw on (modified in place)
            bat_center: (x, y) center of detected bat
            bat_id: Unique identifier for this bat
            polygon_idx: Index of polygon where bat was detected
        """
        cx, cy = bat_center
        
        # === SUBTLE BAT MARKERS FOR CLEAN VISUALIZATION ===
        
        # Primary bat detection circle - smaller and more subtle
        main_radius = 8   # Smaller radius for cleaner look
        center_radius = 3 # Smaller center
        
        # Main bat detection circle with clean green
        cv2.circle(frame, (cx, cy), main_radius, (0, 255, 0), 2)  # Green circle
        cv2.circle(frame, (cx, cy), center_radius, (255, 255, 255), -1)  # White center
        
        # Subtle crosshair for precise center - smaller and cleaner
        cross_size = 6
        cv2.line(frame, (cx - cross_size, cy), (cx + cross_size, cy), (0, 0, 255), 2)  # Red lines
        cv2.line(frame, (cx, cy - cross_size), (cx, cy + cross_size), (0, 0, 255), 2)
        
        # === MINIMAL TEXT LABELING ===
        # Small, clean text that doesn't clutter the frame
        text = f"B{bat_id}"  # Simplified: "B1", "B2", etc.
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.4  # Much smaller font
        font_thickness = 1  # Thinner text
        
        # Position text to the right of the bat to avoid overlap
        text_x = cx + 15
        text_y = cy - 8
        
        # Minimal background for text readability
        (text_width, text_height), _ = cv2.getTextSize(text, font, font_scale, font_thickness)
        
        # Small, unobtrusive background
        bg_padding = 2
        cv2.rectangle(frame, 
                     (text_x - bg_padding, text_y - text_height - bg_padding),
                     (text_x + text_width + bg_padding, text_y + bg_padding),
                     (0, 0, 0), -1)  # Small black background
        
        # Clean white text
        cv2.putText(frame, text, (text_x, text_y), 
                   font, font_scale, (255, 255, 255), font_thickness)
    
    def add_export_watermark(self, frame):
        """
        Add a subtle watermark to exported frames indicating polygon detection mode.
        
        Args:
            frame: Frame to add watermark to (modified in place)
        """
        if not self.polygon_areas:
            return frame
            
        # Add subtle watermark in bottom-right corner
        height, width = frame.shape[:2]
        watermark_text = f"Polygon Detection ({len(self.polygon_areas)} areas)"
        
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 0.5
        font_thickness = 1
        
        # Get text size
        (text_width, text_height), baseline = cv2.getTextSize(watermark_text, font, font_scale, font_thickness)
        
        # Position in bottom-right with padding
        padding = 10
        text_x = width - text_width - padding
        text_y = height - padding
        
        # Semi-transparent background
        overlay = frame.copy()
        cv2.rectangle(overlay, 
                     (text_x - 5, text_y - text_height - 5),
                     (text_x + text_width + 5, text_y + 5),
                     (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # Add text
        cv2.putText(frame, watermark_text, (text_x, text_y),
                   font, font_scale, (255, 255, 255), font_thickness)
        
        return frame